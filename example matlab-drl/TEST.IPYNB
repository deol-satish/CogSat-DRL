{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102df74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.buffers import RolloutBuffer\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# === Setup Pendulum environment ===\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "dummy_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = A2C(\"MlpPolicy\", dummy_env, verbose=0, device=\"cpu\")\n",
    "\n",
    "n_steps = 5\n",
    "buffer = RolloutBuffer(\n",
    "    buffer_size=n_steps,\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    device=model.device,\n",
    "    gamma=model.gamma,\n",
    "    gae_lambda=model.gae_lambda,\n",
    ")\n",
    "\n",
    "# === Globals ===\n",
    "step_count = 0\n",
    "obs_last = None\n",
    "action_last = None\n",
    "value_last = None\n",
    "log_prob_last = None\n",
    "\n",
    "def reset_env():\n",
    "    global obs_last\n",
    "    obs_last = dummy_env.reset()\n",
    "    return obs_last\n",
    "\n",
    "def get_action(obs):\n",
    "    global obs_last, action_last, value_last, log_prob_last\n",
    "    obs_last = np.array(obs, dtype=np.float32).reshape((1, -1))\n",
    "    obs_tensor = torch.tensor(obs_last).float().to(model.device)\n",
    "    with torch.no_grad():\n",
    "        action_tensor, value_tensor, log_prob_tensor = model.policy.forward(obs_tensor)\n",
    "    action_last = action_tensor\n",
    "    value_last = value_tensor\n",
    "    log_prob_last = log_prob_tensor\n",
    "    return action_tensor.cpu().numpy()[0]\n",
    "\n",
    "def my_step(action):\n",
    "    next_obs, reward, terminated, truncated, _ = env.step(np.array(action))\n",
    "    done = terminated or truncated\n",
    "    return next_obs, reward, done\n",
    "\n",
    "def store_transition(reward, done, next_obs):\n",
    "    global step_count, obs_last, action_last, value_last, log_prob_last\n",
    "    reward = np.array([reward], dtype=np.float32)\n",
    "    done = np.array([done], dtype=bool)\n",
    "    next_obs = np.array(next_obs, dtype=np.float32).reshape((1, -1))\n",
    "    buffer.add(obs_last, action_last, reward, done, value_last, log_prob_last)\n",
    "    step_count += 1\n",
    "    obs_last = next_obs\n",
    "    if step_count % n_steps == 0:\n",
    "        with torch.no_grad():\n",
    "            last_val = model.policy.predict_values(torch.tensor(next_obs).float().to(model.device))\n",
    "        buffer.compute_returns_and_advantage(last_val, dones=done)\n",
    "        model.train()\n",
    "\n",
    "def save_model(path=\"a2c_pendulum\"):\n",
    "    model.save(path)\n",
    "\n",
    "def load_model(path=\"a2c_pendulum\"):\n",
    "    global model\n",
    "    model = A2C.load(path)\n",
    "    model.set_env(dummy_env)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568cd2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_a2c_loss(policy, rollout_data, value_coef=0.5, entropy_coef=0.01):\n",
    "    observations = rollout_data.observations\n",
    "    actions = rollout_data.actions\n",
    "    returns = rollout_data.returns\n",
    "    advantages = rollout_data.advantages\n",
    "    old_log_probs = rollout_data.old_log_prob\n",
    "\n",
    "    # Get action distribution and value predictions\n",
    "    dist = policy.get_distribution(observations)\n",
    "    value_preds = policy.predict_values(observations)\n",
    "\n",
    "    # Log probs and entropy from the current policy\n",
    "    new_log_probs = dist.log_prob(actions)\n",
    "    entropy = dist.entropy().mean()\n",
    "\n",
    "    # Actor loss\n",
    "    policy_loss = -(advantages * new_log_probs).mean()\n",
    "\n",
    "    # Critic loss\n",
    "    value_loss = torch.nn.functional.mse_loss(returns, value_preds)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = policy_loss + value_coef * value_loss - entropy_coef * entropy\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e61ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.buffers import RolloutBuffer\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# === Setup Pendulum environment ===\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "dummy_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = A2C(\"MlpPolicy\", dummy_env, verbose=0, device=\"cpu\")\n",
    "\n",
    "n_steps = 5\n",
    "buffer = RolloutBuffer(\n",
    "    buffer_size=n_steps,\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    device=model.device,\n",
    "    gamma=model.gamma,\n",
    "    gae_lambda=model.gae_lambda,\n",
    ")\n",
    "\n",
    "# === Globals ===\n",
    "step_count = 0\n",
    "obs_last = None\n",
    "action_last = None\n",
    "value_last = None\n",
    "log_prob_last = None\n",
    "\n",
    "def reset_env():\n",
    "    global obs_last\n",
    "    obs_last = dummy_env.reset()\n",
    "    return obs_last\n",
    "\n",
    "def get_action(obs):\n",
    "    global obs_last, action_last, value_last, log_prob_last\n",
    "    obs_last = np.array(obs, dtype=np.float32).reshape((1, -1))\n",
    "    obs_tensor = torch.tensor(obs_last).float().to(model.device)\n",
    "    with torch.no_grad():\n",
    "        action_tensor, value_tensor, log_prob_tensor = model.policy.forward(obs_tensor)\n",
    "    action_last = action_tensor\n",
    "    value_last = value_tensor\n",
    "    log_prob_last = log_prob_tensor\n",
    "    return action_tensor.cpu().numpy()[0]\n",
    "\n",
    "def my_step(action):\n",
    "    next_obs, reward, terminated, truncated, _ = env.step(np.array(action))\n",
    "    done = terminated or truncated\n",
    "    return next_obs, reward, done\n",
    "\n",
    "def store_transition(reward, done, next_obs):\n",
    "    global step_count, obs_last, action_last, value_last, log_prob_last\n",
    "    reward = np.array([reward], dtype=np.float32)\n",
    "    done = np.array([done], dtype=bool)\n",
    "    next_obs = np.array(next_obs, dtype=np.float32).reshape((1, -1))\n",
    "    buffer.add(obs_last, action_last, reward, done, value_last, log_prob_last)\n",
    "    step_count += 1\n",
    "    obs_last = next_obs\n",
    "    if step_count % n_steps == 0:\n",
    "        with torch.no_grad():\n",
    "            last_val = model.policy.predict_values(torch.tensor(next_obs).float().to(model.device))\n",
    "\n",
    "        buffer.compute_returns_and_advantage(last_val, dones=done)\n",
    "\n",
    "        # Manual A2C training loop\n",
    "        model.policy.train()\n",
    "        model.policy.optimizer.zero_grad()\n",
    "        for rollout_data in buffer.get(batch_size=None):\n",
    "            loss = compute_a2c_loss(model.policy, rollout_data)\n",
    "            loss.backward()\n",
    "        model.policy.optimizer.step()\n",
    "\n",
    "        buffer.reset()\n",
    "\n",
    "\n",
    "def save_model(path=\"a2c_pendulum\"):\n",
    "    model.save(path)\n",
    "\n",
    "def load_model(path=\"a2c_pendulum\"):\n",
    "    global model\n",
    "    model = A2C.load(path)\n",
    "    model.set_env(dummy_env)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3409f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_last = reset_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdce5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = get_action(obs_last)\n",
    "next_obs, reward, done = my_step(action)\n",
    "store_transition(reward, done, next_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4cb541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step count: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Step count:\", step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa57d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiple_episodes(n_episodes=100):\n",
    "    global step_count\n",
    "    obs = reset_env()\n",
    "    for episode in range(n_episodes):\n",
    "        \n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = get_action(obs)\n",
    "            next_obs, reward, done = my_step(action)\n",
    "            store_transition(reward, done, next_obs)\n",
    "            obs = next_obs\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break  # Optional, since the loop exits on `done` anyway\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}, Total Steps = {step_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23643c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = -765.26, Total Steps = 201\n",
      "Episode 2: Reward = -2.14, Total Steps = 202\n",
      "Episode 3: Reward = -2.81, Total Steps = 203\n",
      "Episode 4: Reward = -3.95, Total Steps = 204\n",
      "Episode 5: Reward = -5.67, Total Steps = 205\n",
      "Episode 6: Reward = -7.94, Total Steps = 206\n",
      "Episode 7: Reward = -10.58, Total Steps = 207\n",
      "Episode 8: Reward = -13.24, Total Steps = 208\n",
      "Episode 9: Reward = -15.75, Total Steps = 209\n",
      "Episode 10: Reward = -14.06, Total Steps = 210\n",
      "Episode 11: Reward = -11.79, Total Steps = 211\n",
      "Episode 12: Reward = -9.42, Total Steps = 212\n",
      "Episode 13: Reward = -7.41, Total Steps = 213\n",
      "Episode 14: Reward = -5.27, Total Steps = 214\n",
      "Episode 15: Reward = -3.87, Total Steps = 215\n",
      "Episode 16: Reward = -2.83, Total Steps = 216\n",
      "Episode 17: Reward = -1.85, Total Steps = 217\n",
      "Episode 18: Reward = -1.28, Total Steps = 218\n",
      "Episode 19: Reward = -0.99, Total Steps = 219\n",
      "Episode 20: Reward = -0.72, Total Steps = 220\n",
      "Episode 21: Reward = -0.53, Total Steps = 221\n",
      "Episode 22: Reward = -0.30, Total Steps = 222\n",
      "Episode 23: Reward = -0.20, Total Steps = 223\n",
      "Episode 24: Reward = -0.15, Total Steps = 224\n",
      "Episode 25: Reward = -0.15, Total Steps = 225\n",
      "Episode 26: Reward = -0.12, Total Steps = 226\n",
      "Episode 27: Reward = -0.10, Total Steps = 227\n",
      "Episode 28: Reward = -0.11, Total Steps = 228\n",
      "Episode 29: Reward = -0.20, Total Steps = 229\n",
      "Episode 30: Reward = -0.37, Total Steps = 230\n",
      "Episode 31: Reward = -0.61, Total Steps = 231\n",
      "Episode 32: Reward = -0.92, Total Steps = 232\n",
      "Episode 33: Reward = -1.40, Total Steps = 233\n",
      "Episode 34: Reward = -1.79, Total Steps = 234\n",
      "Episode 35: Reward = -2.46, Total Steps = 235\n",
      "Episode 36: Reward = -3.57, Total Steps = 236\n",
      "Episode 37: Reward = -4.83, Total Steps = 237\n",
      "Episode 38: Reward = -6.87, Total Steps = 238\n",
      "Episode 39: Reward = -8.66, Total Steps = 239\n",
      "Episode 40: Reward = -11.43, Total Steps = 240\n",
      "Episode 41: Reward = -14.02, Total Steps = 241\n",
      "Episode 42: Reward = -15.16, Total Steps = 242\n",
      "Episode 43: Reward = -13.17, Total Steps = 243\n",
      "Episode 44: Reward = -10.85, Total Steps = 244\n",
      "Episode 45: Reward = -8.48, Total Steps = 245\n",
      "Episode 46: Reward = -6.78, Total Steps = 246\n",
      "Episode 47: Reward = -4.95, Total Steps = 247\n",
      "Episode 48: Reward = -3.48, Total Steps = 248\n",
      "Episode 49: Reward = -2.51, Total Steps = 249\n",
      "Episode 50: Reward = -1.85, Total Steps = 250\n",
      "Episode 51: Reward = -1.29, Total Steps = 251\n",
      "Episode 52: Reward = -1.06, Total Steps = 252\n",
      "Episode 53: Reward = -0.75, Total Steps = 253\n",
      "Episode 54: Reward = -0.76, Total Steps = 254\n",
      "Episode 55: Reward = -0.76, Total Steps = 255\n",
      "Episode 56: Reward = -1.01, Total Steps = 256\n",
      "Episode 57: Reward = -1.06, Total Steps = 257\n",
      "Episode 58: Reward = -1.41, Total Steps = 258\n",
      "Episode 59: Reward = -1.94, Total Steps = 259\n",
      "Episode 60: Reward = -2.82, Total Steps = 260\n",
      "Episode 61: Reward = -3.82, Total Steps = 261\n",
      "Episode 62: Reward = -5.09, Total Steps = 262\n",
      "Episode 63: Reward = -6.87, Total Steps = 263\n",
      "Episode 64: Reward = -9.32, Total Steps = 264\n",
      "Episode 65: Reward = -12.47, Total Steps = 265\n",
      "Episode 66: Reward = -14.90, Total Steps = 266\n",
      "Episode 67: Reward = -15.21, Total Steps = 267\n",
      "Episode 68: Reward = -12.90, Total Steps = 268\n",
      "Episode 69: Reward = -10.79, Total Steps = 269\n",
      "Episode 70: Reward = -8.29, Total Steps = 270\n",
      "Episode 71: Reward = -5.97, Total Steps = 271\n",
      "Episode 72: Reward = -4.43, Total Steps = 272\n",
      "Episode 73: Reward = -3.21, Total Steps = 273\n",
      "Episode 74: Reward = -2.24, Total Steps = 274\n",
      "Episode 75: Reward = -1.68, Total Steps = 275\n",
      "Episode 76: Reward = -1.25, Total Steps = 276\n",
      "Episode 77: Reward = -0.81, Total Steps = 277\n",
      "Episode 78: Reward = -0.48, Total Steps = 278\n",
      "Episode 79: Reward = -0.36, Total Steps = 279\n",
      "Episode 80: Reward = -0.24, Total Steps = 280\n",
      "Episode 81: Reward = -0.17, Total Steps = 281\n",
      "Episode 82: Reward = -0.11, Total Steps = 282\n",
      "Episode 83: Reward = -0.09, Total Steps = 283\n",
      "Episode 84: Reward = -0.11, Total Steps = 284\n",
      "Episode 85: Reward = -0.15, Total Steps = 285\n",
      "Episode 86: Reward = -0.23, Total Steps = 286\n",
      "Episode 87: Reward = -0.34, Total Steps = 287\n",
      "Episode 88: Reward = -0.51, Total Steps = 288\n",
      "Episode 89: Reward = -0.60, Total Steps = 289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deols\\AppData\\Local\\Temp\\ipykernel_38552\\226041759.py:20: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = torch.nn.functional.mse_loss(returns, value_preds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 90: Reward = -0.90, Total Steps = 290\n",
      "Episode 91: Reward = -1.29, Total Steps = 291\n",
      "Episode 92: Reward = -1.92, Total Steps = 292\n",
      "Episode 93: Reward = -2.54, Total Steps = 293\n",
      "Episode 94: Reward = -3.65, Total Steps = 294\n",
      "Episode 95: Reward = -5.09, Total Steps = 295\n",
      "Episode 96: Reward = -7.21, Total Steps = 296\n",
      "Episode 97: Reward = -9.45, Total Steps = 297\n",
      "Episode 98: Reward = -12.07, Total Steps = 298\n",
      "Episode 99: Reward = -15.03, Total Steps = 299\n",
      "Episode 100: Reward = -14.62, Total Steps = 300\n",
      "Episode 101: Reward = -12.59, Total Steps = 301\n",
      "Episode 102: Reward = -10.39, Total Steps = 302\n",
      "Episode 103: Reward = -8.17, Total Steps = 303\n",
      "Episode 104: Reward = -6.04, Total Steps = 304\n",
      "Episode 105: Reward = -4.22, Total Steps = 305\n",
      "Episode 106: Reward = -3.00, Total Steps = 306\n",
      "Episode 107: Reward = -2.30, Total Steps = 307\n",
      "Episode 108: Reward = -1.65, Total Steps = 308\n",
      "Episode 109: Reward = -1.15, Total Steps = 309\n",
      "Episode 110: Reward = -0.89, Total Steps = 310\n",
      "Episode 111: Reward = -0.73, Total Steps = 311\n",
      "Episode 112: Reward = -0.58, Total Steps = 312\n",
      "Episode 113: Reward = -0.63, Total Steps = 313\n",
      "Episode 114: Reward = -0.69, Total Steps = 314\n",
      "Episode 115: Reward = -0.72, Total Steps = 315\n",
      "Episode 116: Reward = -0.90, Total Steps = 316\n",
      "Episode 117: Reward = -1.19, Total Steps = 317\n",
      "Episode 118: Reward = -1.65, Total Steps = 318\n",
      "Episode 119: Reward = -2.27, Total Steps = 319\n",
      "Episode 120: Reward = -3.17, Total Steps = 320\n",
      "Episode 121: Reward = -4.38, Total Steps = 321\n",
      "Episode 122: Reward = -6.04, Total Steps = 322\n",
      "Episode 123: Reward = -8.20, Total Steps = 323\n",
      "Episode 124: Reward = -10.91, Total Steps = 324\n",
      "Episode 125: Reward = -13.65, Total Steps = 325\n",
      "Episode 126: Reward = -16.18, Total Steps = 326\n",
      "Episode 127: Reward = -13.75, Total Steps = 327\n",
      "Episode 128: Reward = -11.58, Total Steps = 328\n",
      "Episode 129: Reward = -8.97, Total Steps = 329\n",
      "Episode 130: Reward = -6.82, Total Steps = 330\n",
      "Episode 131: Reward = -4.94, Total Steps = 331\n",
      "Episode 132: Reward = -3.42, Total Steps = 332\n",
      "Episode 133: Reward = -2.52, Total Steps = 333\n",
      "Episode 134: Reward = -1.67, Total Steps = 334\n",
      "Episode 135: Reward = -1.25, Total Steps = 335\n",
      "Episode 136: Reward = -0.87, Total Steps = 336\n",
      "Episode 137: Reward = -0.71, Total Steps = 337\n",
      "Episode 138: Reward = -0.58, Total Steps = 338\n",
      "Episode 139: Reward = -0.40, Total Steps = 339\n",
      "Episode 140: Reward = -0.34, Total Steps = 340\n",
      "Episode 141: Reward = -0.31, Total Steps = 341\n",
      "Episode 142: Reward = -0.24, Total Steps = 342\n",
      "Episode 143: Reward = -0.17, Total Steps = 343\n",
      "Episode 144: Reward = -0.24, Total Steps = 344\n",
      "Episode 145: Reward = -0.26, Total Steps = 345\n",
      "Episode 146: Reward = -0.34, Total Steps = 346\n",
      "Episode 147: Reward = -0.37, Total Steps = 347\n",
      "Episode 148: Reward = -0.55, Total Steps = 348\n",
      "Episode 149: Reward = -0.79, Total Steps = 349\n",
      "Episode 150: Reward = -1.14, Total Steps = 350\n",
      "Episode 151: Reward = -1.67, Total Steps = 351\n",
      "Episode 152: Reward = -2.33, Total Steps = 352\n",
      "Episode 153: Reward = -3.42, Total Steps = 353\n",
      "Episode 154: Reward = -4.97, Total Steps = 354\n",
      "Episode 155: Reward = -6.71, Total Steps = 355\n",
      "Episode 156: Reward = -8.73, Total Steps = 356\n",
      "Episode 157: Reward = -11.07, Total Steps = 357\n",
      "Episode 158: Reward = -13.47, Total Steps = 358\n",
      "Episode 159: Reward = -14.76, Total Steps = 359\n",
      "Episode 160: Reward = -12.49, Total Steps = 360\n",
      "Episode 161: Reward = -10.35, Total Steps = 361\n",
      "Episode 162: Reward = -8.11, Total Steps = 362\n",
      "Episode 163: Reward = -6.07, Total Steps = 363\n",
      "Episode 164: Reward = -4.58, Total Steps = 364\n",
      "Episode 165: Reward = -3.33, Total Steps = 365\n",
      "Episode 166: Reward = -2.51, Total Steps = 366\n",
      "Episode 167: Reward = -1.82, Total Steps = 367\n",
      "Episode 168: Reward = -1.37, Total Steps = 368\n",
      "Episode 169: Reward = -1.06, Total Steps = 369\n",
      "Episode 170: Reward = -0.83, Total Steps = 370\n",
      "Episode 171: Reward = -0.72, Total Steps = 371\n",
      "Episode 172: Reward = -0.70, Total Steps = 372\n",
      "Episode 173: Reward = -0.76, Total Steps = 373\n",
      "Episode 174: Reward = -0.95, Total Steps = 374\n",
      "Episode 175: Reward = -1.36, Total Steps = 375\n",
      "Episode 176: Reward = -1.84, Total Steps = 376\n",
      "Episode 177: Reward = -2.57, Total Steps = 377\n",
      "Episode 178: Reward = -3.61, Total Steps = 378\n",
      "Episode 179: Reward = -4.86, Total Steps = 379\n",
      "Episode 180: Reward = -6.58, Total Steps = 380\n",
      "Episode 181: Reward = -8.66, Total Steps = 381\n",
      "Episode 182: Reward = -11.04, Total Steps = 382\n",
      "Episode 183: Reward = -14.03, Total Steps = 383\n",
      "Episode 184: Reward = -14.56, Total Steps = 384\n",
      "Episode 185: Reward = -12.17, Total Steps = 385\n",
      "Episode 186: Reward = -10.13, Total Steps = 386\n",
      "Episode 187: Reward = -8.29, Total Steps = 387\n",
      "Episode 188: Reward = -6.33, Total Steps = 388\n",
      "Episode 189: Reward = -4.65, Total Steps = 389\n",
      "Episode 190: Reward = -3.38, Total Steps = 390\n",
      "Episode 191: Reward = -2.41, Total Steps = 391\n",
      "Episode 192: Reward = -1.75, Total Steps = 392\n",
      "Episode 193: Reward = -1.23, Total Steps = 393\n",
      "Episode 194: Reward = -0.85, Total Steps = 394\n",
      "Episode 195: Reward = -0.65, Total Steps = 395\n",
      "Episode 196: Reward = -0.55, Total Steps = 396\n",
      "Episode 197: Reward = -0.43, Total Steps = 397\n",
      "Episode 198: Reward = -0.41, Total Steps = 398\n",
      "Episode 199: Reward = -0.45, Total Steps = 399\n",
      "Episode 200: Reward = -0.43, Total Steps = 400\n",
      "Episode 201: Reward = -0.57, Total Steps = 401\n",
      "Episode 202: Reward = -0.89, Total Steps = 402\n",
      "Episode 203: Reward = -1.16, Total Steps = 403\n",
      "Episode 204: Reward = -1.65, Total Steps = 404\n",
      "Episode 205: Reward = -2.26, Total Steps = 405\n",
      "Episode 206: Reward = -3.05, Total Steps = 406\n",
      "Episode 207: Reward = -4.10, Total Steps = 407\n",
      "Episode 208: Reward = -5.47, Total Steps = 408\n",
      "Episode 209: Reward = -7.30, Total Steps = 409\n",
      "Episode 210: Reward = -9.40, Total Steps = 410\n",
      "Episode 211: Reward = -11.81, Total Steps = 411\n",
      "Episode 212: Reward = -14.50, Total Steps = 412\n",
      "Episode 213: Reward = -14.03, Total Steps = 413\n",
      "Episode 214: Reward = -12.01, Total Steps = 414\n",
      "Episode 215: Reward = -9.72, Total Steps = 415\n",
      "Episode 216: Reward = -7.49, Total Steps = 416\n",
      "Episode 217: Reward = -5.74, Total Steps = 417\n",
      "Episode 218: Reward = -4.29, Total Steps = 418\n",
      "Episode 219: Reward = -3.10, Total Steps = 419\n",
      "Episode 220: Reward = -2.21, Total Steps = 420\n",
      "Episode 221: Reward = -1.52, Total Steps = 421\n",
      "Episode 222: Reward = -0.93, Total Steps = 422\n",
      "Episode 223: Reward = -0.68, Total Steps = 423\n",
      "Episode 224: Reward = -0.51, Total Steps = 424\n",
      "Episode 225: Reward = -0.34, Total Steps = 425\n",
      "Episode 226: Reward = -0.25, Total Steps = 426\n",
      "Episode 227: Reward = -0.18, Total Steps = 427\n",
      "Episode 228: Reward = -0.12, Total Steps = 428\n",
      "Episode 229: Reward = -0.11, Total Steps = 429\n",
      "Episode 230: Reward = -0.11, Total Steps = 430\n",
      "Episode 231: Reward = -0.13, Total Steps = 431\n",
      "Episode 232: Reward = -0.20, Total Steps = 432\n",
      "Episode 233: Reward = -0.28, Total Steps = 433\n",
      "Episode 234: Reward = -0.42, Total Steps = 434\n",
      "Episode 235: Reward = -0.63, Total Steps = 435\n",
      "Episode 236: Reward = -0.94, Total Steps = 436\n",
      "Episode 237: Reward = -1.24, Total Steps = 437\n",
      "Episode 238: Reward = -1.90, Total Steps = 438\n",
      "Episode 239: Reward = -2.77, Total Steps = 439\n",
      "Episode 240: Reward = -3.78, Total Steps = 440\n",
      "Episode 241: Reward = -5.15, Total Steps = 441\n",
      "Episode 242: Reward = -6.88, Total Steps = 442\n",
      "Episode 243: Reward = -9.13, Total Steps = 443\n",
      "Episode 244: Reward = -11.61, Total Steps = 444\n",
      "Episode 245: Reward = -14.51, Total Steps = 445\n",
      "Episode 246: Reward = -14.24, Total Steps = 446\n",
      "Episode 247: Reward = -12.20, Total Steps = 447\n",
      "Episode 248: Reward = -9.57, Total Steps = 448\n",
      "Episode 249: Reward = -7.35, Total Steps = 449\n",
      "Episode 250: Reward = -5.52, Total Steps = 450\n",
      "Episode 251: Reward = -3.92, Total Steps = 451\n",
      "Episode 252: Reward = -2.88, Total Steps = 452\n",
      "Episode 253: Reward = -2.10, Total Steps = 453\n",
      "Episode 254: Reward = -1.49, Total Steps = 454\n",
      "Episode 255: Reward = -1.17, Total Steps = 455\n",
      "Episode 256: Reward = -1.01, Total Steps = 456\n",
      "Episode 257: Reward = -0.94, Total Steps = 457\n",
      "Episode 258: Reward = -0.97, Total Steps = 458\n",
      "Episode 259: Reward = -1.09, Total Steps = 459\n",
      "Episode 260: Reward = -1.38, Total Steps = 460\n",
      "Episode 261: Reward = -1.71, Total Steps = 461\n",
      "Episode 262: Reward = -2.20, Total Steps = 462\n",
      "Episode 263: Reward = -3.15, Total Steps = 463\n",
      "Episode 264: Reward = -4.32, Total Steps = 464\n",
      "Episode 265: Reward = -5.83, Total Steps = 465\n",
      "Episode 266: Reward = -7.54, Total Steps = 466\n",
      "Episode 267: Reward = -9.35, Total Steps = 467\n",
      "Episode 268: Reward = -11.53, Total Steps = 468\n",
      "Episode 269: Reward = -13.68, Total Steps = 469\n",
      "Episode 270: Reward = -12.54, Total Steps = 470\n",
      "Episode 271: Reward = -10.42, Total Steps = 471\n",
      "Episode 272: Reward = -8.53, Total Steps = 472\n",
      "Episode 273: Reward = -6.72, Total Steps = 473\n",
      "Episode 274: Reward = -5.28, Total Steps = 474\n",
      "Episode 275: Reward = -4.03, Total Steps = 475\n",
      "Episode 276: Reward = -3.00, Total Steps = 476\n",
      "Episode 277: Reward = -2.25, Total Steps = 477\n",
      "Episode 278: Reward = -1.81, Total Steps = 478\n",
      "Episode 279: Reward = -1.53, Total Steps = 479\n",
      "Episode 280: Reward = -1.53, Total Steps = 480\n",
      "Episode 281: Reward = -1.75, Total Steps = 481\n",
      "Episode 282: Reward = -2.21, Total Steps = 482\n",
      "Episode 283: Reward = -2.87, Total Steps = 483\n",
      "Episode 284: Reward = -3.95, Total Steps = 484\n",
      "Episode 285: Reward = -5.04, Total Steps = 485\n",
      "Episode 286: Reward = -6.52, Total Steps = 486\n",
      "Episode 287: Reward = -8.33, Total Steps = 487\n",
      "Episode 288: Reward = -10.42, Total Steps = 488\n",
      "Episode 289: Reward = -12.20, Total Steps = 489\n",
      "Episode 290: Reward = -12.91, Total Steps = 490\n",
      "Episode 291: Reward = -10.97, Total Steps = 491\n",
      "Episode 292: Reward = -9.02, Total Steps = 492\n",
      "Episode 293: Reward = -7.46, Total Steps = 493\n",
      "Episode 294: Reward = -5.94, Total Steps = 494\n",
      "Episode 295: Reward = -4.62, Total Steps = 495\n",
      "Episode 296: Reward = -3.58, Total Steps = 496\n",
      "Episode 297: Reward = -2.93, Total Steps = 497\n",
      "Episode 298: Reward = -2.38, Total Steps = 498\n",
      "Episode 299: Reward = -2.08, Total Steps = 499\n",
      "Episode 300: Reward = -2.03, Total Steps = 500\n",
      "Episode 301: Reward = -2.24, Total Steps = 501\n",
      "Episode 302: Reward = -2.74, Total Steps = 502\n",
      "Episode 303: Reward = -3.44, Total Steps = 503\n",
      "Episode 304: Reward = -4.32, Total Steps = 504\n",
      "Episode 305: Reward = -5.76, Total Steps = 505\n",
      "Episode 306: Reward = -7.29, Total Steps = 506\n",
      "Episode 307: Reward = -9.13, Total Steps = 507\n",
      "Episode 308: Reward = -10.69, Total Steps = 508\n",
      "Episode 309: Reward = -12.67, Total Steps = 509\n",
      "Episode 310: Reward = -11.64, Total Steps = 510\n",
      "Episode 311: Reward = -9.84, Total Steps = 511\n",
      "Episode 312: Reward = -8.04, Total Steps = 512\n",
      "Episode 313: Reward = -6.68, Total Steps = 513\n",
      "Episode 314: Reward = -5.46, Total Steps = 514\n",
      "Episode 315: Reward = -4.45, Total Steps = 515\n",
      "Episode 316: Reward = -3.75, Total Steps = 516\n",
      "Episode 317: Reward = -3.23, Total Steps = 517\n",
      "Episode 318: Reward = -2.88, Total Steps = 518\n",
      "Episode 319: Reward = -2.73, Total Steps = 519\n",
      "Episode 320: Reward = -2.79, Total Steps = 520\n",
      "Episode 321: Reward = -3.13, Total Steps = 521\n",
      "Episode 322: Reward = -3.61, Total Steps = 522\n",
      "Episode 323: Reward = -4.44, Total Steps = 523\n",
      "Episode 324: Reward = -5.46, Total Steps = 524\n",
      "Episode 325: Reward = -6.69, Total Steps = 525\n",
      "Episode 326: Reward = -8.09, Total Steps = 526\n",
      "Episode 327: Reward = -9.62, Total Steps = 527\n",
      "Episode 328: Reward = -11.53, Total Steps = 528\n",
      "Episode 329: Reward = -11.97, Total Steps = 529\n",
      "Episode 330: Reward = -10.54, Total Steps = 530\n",
      "Episode 331: Reward = -8.84, Total Steps = 531\n",
      "Episode 332: Reward = -7.18, Total Steps = 532\n",
      "Episode 333: Reward = -5.96, Total Steps = 533\n",
      "Episode 334: Reward = -4.83, Total Steps = 534\n",
      "Episode 335: Reward = -3.85, Total Steps = 535\n",
      "Episode 336: Reward = -3.19, Total Steps = 536\n",
      "Episode 337: Reward = -2.74, Total Steps = 537\n",
      "Episode 338: Reward = -2.62, Total Steps = 538\n",
      "Episode 339: Reward = -2.78, Total Steps = 539\n",
      "Episode 340: Reward = -3.28, Total Steps = 540\n",
      "Episode 341: Reward = -3.82, Total Steps = 541\n",
      "Episode 342: Reward = -4.88, Total Steps = 542\n",
      "Episode 343: Reward = -6.18, Total Steps = 543\n",
      "Episode 344: Reward = -7.55, Total Steps = 544\n",
      "Episode 345: Reward = -9.36, Total Steps = 545\n",
      "Episode 346: Reward = -11.25, Total Steps = 546\n",
      "Episode 347: Reward = -12.70, Total Steps = 547\n",
      "Episode 348: Reward = -10.86, Total Steps = 548\n",
      "Episode 349: Reward = -9.19, Total Steps = 549\n",
      "Episode 350: Reward = -7.59, Total Steps = 550\n",
      "Episode 351: Reward = -6.12, Total Steps = 551\n",
      "Episode 352: Reward = -5.11, Total Steps = 552\n",
      "Episode 353: Reward = -4.29, Total Steps = 553\n",
      "Episode 354: Reward = -3.60, Total Steps = 554\n",
      "Episode 355: Reward = -3.10, Total Steps = 555\n",
      "Episode 356: Reward = -2.89, Total Steps = 556\n",
      "Episode 357: Reward = -3.02, Total Steps = 557\n",
      "Episode 358: Reward = -3.38, Total Steps = 558\n",
      "Episode 359: Reward = -4.08, Total Steps = 559\n",
      "Episode 360: Reward = -5.09, Total Steps = 560\n",
      "Episode 361: Reward = -6.30, Total Steps = 561\n",
      "Episode 362: Reward = -7.99, Total Steps = 562\n",
      "Episode 363: Reward = -9.88, Total Steps = 563\n",
      "Episode 364: Reward = -11.87, Total Steps = 564\n",
      "Episode 365: Reward = -12.35, Total Steps = 565\n",
      "Episode 366: Reward = -10.53, Total Steps = 566\n",
      "Episode 367: Reward = -8.74, Total Steps = 567\n",
      "Episode 368: Reward = -7.02, Total Steps = 568\n",
      "Episode 369: Reward = -5.84, Total Steps = 569\n",
      "Episode 370: Reward = -4.62, Total Steps = 570\n",
      "Episode 371: Reward = -3.84, Total Steps = 571\n",
      "Episode 372: Reward = -3.32, Total Steps = 572\n",
      "Episode 373: Reward = -3.12, Total Steps = 573\n",
      "Episode 374: Reward = -3.17, Total Steps = 574\n",
      "Episode 375: Reward = -3.49, Total Steps = 575\n",
      "Episode 376: Reward = -4.15, Total Steps = 576\n",
      "Episode 377: Reward = -4.89, Total Steps = 577\n",
      "Episode 378: Reward = -5.91, Total Steps = 578\n",
      "Episode 379: Reward = -7.13, Total Steps = 579\n",
      "Episode 380: Reward = -8.73, Total Steps = 580\n",
      "Episode 381: Reward = -10.15, Total Steps = 581\n",
      "Episode 382: Reward = -11.71, Total Steps = 582\n",
      "Episode 383: Reward = -10.92, Total Steps = 583\n",
      "Episode 384: Reward = -9.72, Total Steps = 584\n",
      "Episode 385: Reward = -8.29, Total Steps = 585\n",
      "Episode 386: Reward = -6.91, Total Steps = 586\n",
      "Episode 387: Reward = -5.64, Total Steps = 587\n",
      "Episode 388: Reward = -4.80, Total Steps = 588\n",
      "Episode 389: Reward = -4.18, Total Steps = 589\n",
      "Episode 390: Reward = -3.76, Total Steps = 590\n",
      "Episode 391: Reward = -3.58, Total Steps = 591\n",
      "Episode 392: Reward = -3.68, Total Steps = 592\n",
      "Episode 393: Reward = -3.93, Total Steps = 593\n",
      "Episode 394: Reward = -4.28, Total Steps = 594\n",
      "Episode 395: Reward = -4.89, Total Steps = 595\n",
      "Episode 396: Reward = -5.57, Total Steps = 596\n",
      "Episode 397: Reward = -6.58, Total Steps = 597\n",
      "Episode 398: Reward = -7.53, Total Steps = 598\n",
      "Episode 399: Reward = -8.51, Total Steps = 599\n",
      "Episode 400: Reward = -9.69, Total Steps = 600\n",
      "Episode 401: Reward = -10.68, Total Steps = 601\n",
      "Episode 402: Reward = -10.45, Total Steps = 602\n",
      "Episode 403: Reward = -9.44, Total Steps = 603\n",
      "Episode 404: Reward = -8.40, Total Steps = 604\n",
      "Episode 405: Reward = -7.40, Total Steps = 605\n",
      "Episode 406: Reward = -6.45, Total Steps = 606\n",
      "Episode 407: Reward = -5.62, Total Steps = 607\n",
      "Episode 408: Reward = -5.07, Total Steps = 608\n",
      "Episode 409: Reward = -4.74, Total Steps = 609\n",
      "Episode 410: Reward = -4.57, Total Steps = 610\n",
      "Episode 411: Reward = -4.58, Total Steps = 611\n",
      "Episode 412: Reward = -4.74, Total Steps = 612\n",
      "Episode 413: Reward = -5.02, Total Steps = 613\n",
      "Episode 414: Reward = -5.49, Total Steps = 614\n",
      "Episode 415: Reward = -5.98, Total Steps = 615\n",
      "Episode 416: Reward = -6.59, Total Steps = 616\n",
      "Episode 417: Reward = -7.46, Total Steps = 617\n",
      "Episode 418: Reward = -8.24, Total Steps = 618\n",
      "Episode 419: Reward = -9.22, Total Steps = 619\n",
      "Episode 420: Reward = -10.13, Total Steps = 620\n",
      "Episode 421: Reward = -10.47, Total Steps = 621\n",
      "Episode 422: Reward = -9.54, Total Steps = 622\n",
      "Episode 423: Reward = -8.62, Total Steps = 623\n",
      "Episode 424: Reward = -7.91, Total Steps = 624\n",
      "Episode 425: Reward = -7.07, Total Steps = 625\n",
      "Episode 426: Reward = -6.50, Total Steps = 626\n",
      "Episode 427: Reward = -6.07, Total Steps = 627\n",
      "Episode 428: Reward = -5.90, Total Steps = 628\n",
      "Episode 429: Reward = -5.94, Total Steps = 629\n",
      "Episode 430: Reward = -6.18, Total Steps = 630\n",
      "Episode 431: Reward = -6.58, Total Steps = 631\n",
      "Episode 432: Reward = -7.26, Total Steps = 632\n",
      "Episode 433: Reward = -7.89, Total Steps = 633\n",
      "Episode 434: Reward = -8.69, Total Steps = 634\n",
      "Episode 435: Reward = -9.54, Total Steps = 635\n",
      "Episode 436: Reward = -10.26, Total Steps = 636\n",
      "Episode 437: Reward = -9.97, Total Steps = 637\n",
      "Episode 438: Reward = -9.22, Total Steps = 638\n",
      "Episode 439: Reward = -8.43, Total Steps = 639\n",
      "Episode 440: Reward = -7.70, Total Steps = 640\n",
      "Episode 441: Reward = -7.00, Total Steps = 641\n",
      "Episode 442: Reward = -6.41, Total Steps = 642\n",
      "Episode 443: Reward = -6.03, Total Steps = 643\n",
      "Episode 444: Reward = -5.75, Total Steps = 644\n",
      "Episode 445: Reward = -5.64, Total Steps = 645\n",
      "Episode 446: Reward = -5.65, Total Steps = 646\n",
      "Episode 447: Reward = -5.91, Total Steps = 647\n",
      "Episode 448: Reward = -6.42, Total Steps = 648\n",
      "Episode 449: Reward = -7.13, Total Steps = 649\n",
      "Episode 450: Reward = -7.83, Total Steps = 650\n",
      "Episode 451: Reward = -8.64, Total Steps = 651\n",
      "Episode 452: Reward = -9.74, Total Steps = 652\n",
      "Episode 453: Reward = -10.81, Total Steps = 653\n",
      "Episode 454: Reward = -9.90, Total Steps = 654\n",
      "Episode 455: Reward = -9.05, Total Steps = 655\n",
      "Episode 456: Reward = -8.18, Total Steps = 656\n",
      "Episode 457: Reward = -7.24, Total Steps = 657\n",
      "Episode 458: Reward = -6.45, Total Steps = 658\n",
      "Episode 459: Reward = -5.83, Total Steps = 659\n",
      "Episode 460: Reward = -5.31, Total Steps = 660\n",
      "Episode 461: Reward = -4.92, Total Steps = 661\n",
      "Episode 462: Reward = -4.68, Total Steps = 662\n",
      "Episode 463: Reward = -4.63, Total Steps = 663\n",
      "Episode 464: Reward = -4.79, Total Steps = 664\n",
      "Episode 465: Reward = -5.22, Total Steps = 665\n",
      "Episode 466: Reward = -5.87, Total Steps = 666\n",
      "Episode 467: Reward = -6.93, Total Steps = 667\n",
      "Episode 468: Reward = -7.81, Total Steps = 668\n",
      "Episode 469: Reward = -8.85, Total Steps = 669\n",
      "Episode 470: Reward = -10.11, Total Steps = 670\n",
      "Episode 471: Reward = -11.12, Total Steps = 671\n",
      "Episode 472: Reward = -10.04, Total Steps = 672\n",
      "Episode 473: Reward = -8.91, Total Steps = 673\n",
      "Episode 474: Reward = -7.82, Total Steps = 674\n",
      "Episode 475: Reward = -6.88, Total Steps = 675\n",
      "Episode 476: Reward = -6.06, Total Steps = 676\n",
      "Episode 477: Reward = -5.45, Total Steps = 677\n",
      "Episode 478: Reward = -5.06, Total Steps = 678\n",
      "Episode 479: Reward = -4.77, Total Steps = 679\n",
      "Episode 480: Reward = -4.66, Total Steps = 680\n",
      "Episode 481: Reward = -4.80, Total Steps = 681\n",
      "Episode 482: Reward = -5.13, Total Steps = 682\n",
      "Episode 483: Reward = -5.68, Total Steps = 683\n",
      "Episode 484: Reward = -6.24, Total Steps = 684\n",
      "Episode 485: Reward = -7.13, Total Steps = 685\n",
      "Episode 486: Reward = -8.14, Total Steps = 686\n",
      "Episode 487: Reward = -9.18, Total Steps = 687\n",
      "Episode 488: Reward = -10.28, Total Steps = 688\n",
      "Episode 489: Reward = -10.81, Total Steps = 689\n",
      "Episode 490: Reward = -9.58, Total Steps = 690\n",
      "Episode 491: Reward = -8.51, Total Steps = 691\n",
      "Episode 492: Reward = -7.62, Total Steps = 692\n",
      "Episode 493: Reward = -6.80, Total Steps = 693\n",
      "Episode 494: Reward = -6.15, Total Steps = 694\n",
      "Episode 495: Reward = -5.74, Total Steps = 695\n",
      "Episode 496: Reward = -5.48, Total Steps = 696\n",
      "Episode 497: Reward = -5.35, Total Steps = 697\n",
      "Episode 498: Reward = -5.34, Total Steps = 698\n",
      "Episode 499: Reward = -5.52, Total Steps = 699\n",
      "Episode 500: Reward = -5.90, Total Steps = 700\n",
      "Episode 501: Reward = -6.41, Total Steps = 701\n",
      "Episode 502: Reward = -7.12, Total Steps = 702\n",
      "Episode 503: Reward = -7.90, Total Steps = 703\n",
      "Episode 504: Reward = -8.71, Total Steps = 704\n",
      "Episode 505: Reward = -9.74, Total Steps = 705\n",
      "Episode 506: Reward = -10.90, Total Steps = 706\n",
      "Episode 507: Reward = -9.86, Total Steps = 707\n",
      "Episode 508: Reward = -8.86, Total Steps = 708\n",
      "Episode 509: Reward = -8.09, Total Steps = 709\n",
      "Episode 510: Reward = -7.35, Total Steps = 710\n",
      "Episode 511: Reward = -6.83, Total Steps = 711\n",
      "Episode 512: Reward = -6.46, Total Steps = 712\n",
      "Episode 513: Reward = -6.32, Total Steps = 713\n",
      "Episode 514: Reward = -6.46, Total Steps = 714\n",
      "Episode 515: Reward = -6.78, Total Steps = 715\n",
      "Episode 516: Reward = -7.28, Total Steps = 716\n",
      "Episode 517: Reward = -7.92, Total Steps = 717\n",
      "Episode 518: Reward = -8.63, Total Steps = 718\n",
      "Episode 519: Reward = -9.27, Total Steps = 719\n",
      "Episode 520: Reward = -9.94, Total Steps = 720\n",
      "Episode 521: Reward = -10.07, Total Steps = 721\n",
      "Episode 522: Reward = -9.43, Total Steps = 722\n",
      "Episode 523: Reward = -8.76, Total Steps = 723\n",
      "Episode 524: Reward = -8.26, Total Steps = 724\n",
      "Episode 525: Reward = -7.84, Total Steps = 725\n",
      "Episode 526: Reward = -7.52, Total Steps = 726\n",
      "Episode 527: Reward = -7.36, Total Steps = 727\n",
      "Episode 528: Reward = -7.34, Total Steps = 728\n",
      "Episode 529: Reward = -7.41, Total Steps = 729\n",
      "Episode 530: Reward = -7.64, Total Steps = 730\n",
      "Episode 531: Reward = -8.00, Total Steps = 731\n",
      "Episode 532: Reward = -8.51, Total Steps = 732\n",
      "Episode 533: Reward = -8.99, Total Steps = 733\n",
      "Episode 534: Reward = -9.41, Total Steps = 734\n",
      "Episode 535: Reward = -9.84, Total Steps = 735\n",
      "Episode 536: Reward = -9.86, Total Steps = 736\n",
      "Episode 537: Reward = -9.38, Total Steps = 737\n",
      "Episode 538: Reward = -8.95, Total Steps = 738\n",
      "Episode 539: Reward = -8.59, Total Steps = 739\n",
      "Episode 540: Reward = -8.37, Total Steps = 740\n",
      "Episode 541: Reward = -8.18, Total Steps = 741\n",
      "Episode 542: Reward = -8.00, Total Steps = 742\n",
      "Episode 543: Reward = -7.84, Total Steps = 743\n",
      "Episode 544: Reward = -7.83, Total Steps = 744\n",
      "Episode 545: Reward = -7.82, Total Steps = 745\n",
      "Episode 546: Reward = -7.84, Total Steps = 746\n",
      "Episode 547: Reward = -8.06, Total Steps = 747\n",
      "Episode 548: Reward = -8.33, Total Steps = 748\n",
      "Episode 549: Reward = -8.60, Total Steps = 749\n",
      "Episode 550: Reward = -8.95, Total Steps = 750\n",
      "Episode 551: Reward = -9.38, Total Steps = 751\n",
      "Episode 552: Reward = -9.90, Total Steps = 752\n",
      "Episode 553: Reward = -9.82, Total Steps = 753\n",
      "Episode 554: Reward = -9.37, Total Steps = 754\n",
      "Episode 555: Reward = -9.00, Total Steps = 755\n",
      "Episode 556: Reward = -8.64, Total Steps = 756\n",
      "Episode 557: Reward = -8.36, Total Steps = 757\n",
      "Episode 558: Reward = -8.10, Total Steps = 758\n",
      "Episode 559: Reward = -7.85, Total Steps = 759\n",
      "Episode 560: Reward = -7.64, Total Steps = 760\n",
      "Episode 561: Reward = -7.54, Total Steps = 761\n",
      "Episode 562: Reward = -7.54, Total Steps = 762\n",
      "Episode 563: Reward = -7.60, Total Steps = 763\n",
      "Episode 564: Reward = -7.71, Total Steps = 764\n",
      "Episode 565: Reward = -7.92, Total Steps = 765\n",
      "Episode 566: Reward = -8.24, Total Steps = 766\n",
      "Episode 567: Reward = -8.66, Total Steps = 767\n",
      "Episode 568: Reward = -9.06, Total Steps = 768\n",
      "Episode 569: Reward = -9.41, Total Steps = 769\n",
      "Episode 570: Reward = -9.80, Total Steps = 770\n",
      "Episode 571: Reward = -9.87, Total Steps = 771\n",
      "Episode 572: Reward = -9.50, Total Steps = 772\n",
      "Episode 573: Reward = -9.13, Total Steps = 773\n",
      "Episode 574: Reward = -8.78, Total Steps = 774\n",
      "Episode 575: Reward = -8.49, Total Steps = 775\n",
      "Episode 576: Reward = -8.24, Total Steps = 776\n",
      "Episode 577: Reward = -8.07, Total Steps = 777\n",
      "Episode 578: Reward = -8.02, Total Steps = 778\n",
      "Episode 579: Reward = -8.05, Total Steps = 779\n",
      "Episode 580: Reward = -8.11, Total Steps = 780\n",
      "Episode 581: Reward = -8.28, Total Steps = 781\n",
      "Episode 582: Reward = -8.59, Total Steps = 782\n",
      "Episode 583: Reward = -9.09, Total Steps = 783\n",
      "Episode 584: Reward = -9.47, Total Steps = 784\n",
      "Episode 585: Reward = -10.02, Total Steps = 785\n",
      "Episode 586: Reward = -9.72, Total Steps = 786\n",
      "Episode 587: Reward = -9.32, Total Steps = 787\n",
      "Episode 588: Reward = -9.01, Total Steps = 788\n",
      "Episode 589: Reward = -8.79, Total Steps = 789\n",
      "Episode 590: Reward = -8.59, Total Steps = 790\n",
      "Episode 591: Reward = -8.43, Total Steps = 791\n",
      "Episode 592: Reward = -8.39, Total Steps = 792\n",
      "Episode 593: Reward = -8.41, Total Steps = 793\n",
      "Episode 594: Reward = -8.54, Total Steps = 794\n",
      "Episode 595: Reward = -8.73, Total Steps = 795\n",
      "Episode 596: Reward = -9.04, Total Steps = 796\n",
      "Episode 597: Reward = -9.30, Total Steps = 797\n",
      "Episode 598: Reward = -9.69, Total Steps = 798\n",
      "Episode 599: Reward = -9.94, Total Steps = 799\n",
      "Episode 600: Reward = -9.55, Total Steps = 800\n",
      "Episode 601: Reward = -9.22, Total Steps = 801\n",
      "Episode 602: Reward = -8.99, Total Steps = 802\n",
      "Episode 603: Reward = -8.84, Total Steps = 803\n",
      "Episode 604: Reward = -8.72, Total Steps = 804\n",
      "Episode 605: Reward = -8.66, Total Steps = 805\n",
      "Episode 606: Reward = -8.62, Total Steps = 806\n",
      "Episode 607: Reward = -8.54, Total Steps = 807\n",
      "Episode 608: Reward = -8.58, Total Steps = 808\n",
      "Episode 609: Reward = -8.67, Total Steps = 809\n",
      "Episode 610: Reward = -8.73, Total Steps = 810\n",
      "Episode 611: Reward = -8.88, Total Steps = 811\n",
      "Episode 612: Reward = -9.08, Total Steps = 812\n",
      "Episode 613: Reward = -9.41, Total Steps = 813\n",
      "Episode 614: Reward = -9.58, Total Steps = 814\n",
      "Episode 615: Reward = -9.80, Total Steps = 815\n",
      "Episode 616: Reward = -9.86, Total Steps = 816\n",
      "Episode 617: Reward = -9.71, Total Steps = 817\n",
      "Episode 618: Reward = -9.56, Total Steps = 818\n",
      "Episode 619: Reward = -9.46, Total Steps = 819\n",
      "Episode 620: Reward = -9.39, Total Steps = 820\n",
      "Episode 621: Reward = -9.28, Total Steps = 821\n",
      "Episode 622: Reward = -9.14, Total Steps = 822\n",
      "Episode 623: Reward = -8.99, Total Steps = 823\n",
      "Episode 624: Reward = -8.84, Total Steps = 824\n",
      "Episode 625: Reward = -8.68, Total Steps = 825\n",
      "Episode 626: Reward = -8.52, Total Steps = 826\n",
      "Episode 627: Reward = -8.43, Total Steps = 827\n",
      "Episode 628: Reward = -8.37, Total Steps = 828\n",
      "Episode 629: Reward = -8.41, Total Steps = 829\n",
      "Episode 630: Reward = -8.51, Total Steps = 830\n",
      "Episode 631: Reward = -8.73, Total Steps = 831\n",
      "Episode 632: Reward = -9.01, Total Steps = 832\n",
      "Episode 633: Reward = -9.46, Total Steps = 833\n",
      "Episode 634: Reward = -9.89, Total Steps = 834\n",
      "Episode 635: Reward = -9.79, Total Steps = 835\n",
      "Episode 636: Reward = -9.41, Total Steps = 836\n",
      "Episode 637: Reward = -8.98, Total Steps = 837\n",
      "Episode 638: Reward = -8.56, Total Steps = 838\n",
      "Episode 639: Reward = -8.21, Total Steps = 839\n",
      "Episode 640: Reward = -7.94, Total Steps = 840\n",
      "Episode 641: Reward = -7.76, Total Steps = 841\n",
      "Episode 642: Reward = -7.63, Total Steps = 842\n",
      "Episode 643: Reward = -7.57, Total Steps = 843\n",
      "Episode 644: Reward = -7.57, Total Steps = 844\n",
      "Episode 645: Reward = -7.61, Total Steps = 845\n",
      "Episode 646: Reward = -7.74, Total Steps = 846\n",
      "Episode 647: Reward = -7.89, Total Steps = 847\n",
      "Episode 648: Reward = -8.13, Total Steps = 848\n",
      "Episode 649: Reward = -8.47, Total Steps = 849\n",
      "Episode 650: Reward = -8.97, Total Steps = 850\n",
      "Episode 651: Reward = -9.51, Total Steps = 851\n",
      "Episode 652: Reward = -10.04, Total Steps = 852\n",
      "Episode 653: Reward = -9.74, Total Steps = 853\n",
      "Episode 654: Reward = -9.27, Total Steps = 854\n",
      "Episode 655: Reward = -8.81, Total Steps = 855\n",
      "Episode 656: Reward = -8.33, Total Steps = 856\n",
      "Episode 657: Reward = -7.95, Total Steps = 857\n",
      "Episode 658: Reward = -7.67, Total Steps = 858\n",
      "Episode 659: Reward = -7.46, Total Steps = 859\n",
      "Episode 660: Reward = -7.34, Total Steps = 860\n",
      "Episode 661: Reward = -7.27, Total Steps = 861\n",
      "Episode 662: Reward = -7.20, Total Steps = 862\n",
      "Episode 663: Reward = -7.26, Total Steps = 863\n",
      "Episode 664: Reward = -7.46, Total Steps = 864\n",
      "Episode 665: Reward = -7.79, Total Steps = 865\n",
      "Episode 666: Reward = -8.12, Total Steps = 866\n",
      "Episode 667: Reward = -8.47, Total Steps = 867\n",
      "Episode 668: Reward = -8.88, Total Steps = 868\n",
      "Episode 669: Reward = -9.47, Total Steps = 869\n",
      "Episode 670: Reward = -9.99, Total Steps = 870\n",
      "Episode 671: Reward = -9.80, Total Steps = 871\n",
      "Episode 672: Reward = -9.23, Total Steps = 872\n",
      "Episode 673: Reward = -8.67, Total Steps = 873\n",
      "Episode 674: Reward = -8.21, Total Steps = 874\n",
      "Episode 675: Reward = -7.80, Total Steps = 875\n",
      "Episode 676: Reward = -7.57, Total Steps = 876\n",
      "Episode 677: Reward = -7.49, Total Steps = 877\n",
      "Episode 678: Reward = -7.50, Total Steps = 878\n",
      "Episode 679: Reward = -7.56, Total Steps = 879\n",
      "Episode 680: Reward = -7.72, Total Steps = 880\n",
      "Episode 681: Reward = -8.01, Total Steps = 881\n",
      "Episode 682: Reward = -8.37, Total Steps = 882\n",
      "Episode 683: Reward = -8.82, Total Steps = 883\n",
      "Episode 684: Reward = -9.19, Total Steps = 884\n",
      "Episode 685: Reward = -9.72, Total Steps = 885\n",
      "Episode 686: Reward = -10.01, Total Steps = 886\n",
      "Episode 687: Reward = -9.57, Total Steps = 887\n",
      "Episode 688: Reward = -9.16, Total Steps = 888\n",
      "Episode 689: Reward = -8.82, Total Steps = 889\n",
      "Episode 690: Reward = -8.53, Total Steps = 890\n",
      "Episode 691: Reward = -8.31, Total Steps = 891\n",
      "Episode 692: Reward = -8.14, Total Steps = 892\n",
      "Episode 693: Reward = -8.06, Total Steps = 893\n",
      "Episode 694: Reward = -8.11, Total Steps = 894\n",
      "Episode 695: Reward = -8.16, Total Steps = 895\n",
      "Episode 696: Reward = -8.32, Total Steps = 896\n",
      "Episode 697: Reward = -8.56, Total Steps = 897\n",
      "Episode 698: Reward = -8.83, Total Steps = 898\n",
      "Episode 699: Reward = -9.08, Total Steps = 899\n",
      "Episode 700: Reward = -9.24, Total Steps = 900\n",
      "Episode 701: Reward = -9.48, Total Steps = 901\n",
      "Episode 702: Reward = -9.83, Total Steps = 902\n",
      "Episode 703: Reward = -9.78, Total Steps = 903\n",
      "Episode 704: Reward = -9.42, Total Steps = 904\n",
      "Episode 705: Reward = -9.08, Total Steps = 905\n",
      "Episode 706: Reward = -8.79, Total Steps = 906\n",
      "Episode 707: Reward = -8.55, Total Steps = 907\n",
      "Episode 708: Reward = -8.37, Total Steps = 908\n",
      "Episode 709: Reward = -8.27, Total Steps = 909\n",
      "Episode 710: Reward = -8.23, Total Steps = 910\n",
      "Episode 711: Reward = -8.21, Total Steps = 911\n",
      "Episode 712: Reward = -8.23, Total Steps = 912\n",
      "Episode 713: Reward = -8.22, Total Steps = 913\n",
      "Episode 714: Reward = -8.30, Total Steps = 914\n",
      "Episode 715: Reward = -8.49, Total Steps = 915\n",
      "Episode 716: Reward = -8.76, Total Steps = 916\n",
      "Episode 717: Reward = -9.09, Total Steps = 917\n",
      "Episode 718: Reward = -9.47, Total Steps = 918\n",
      "Episode 719: Reward = -9.80, Total Steps = 919\n",
      "Episode 720: Reward = -9.86, Total Steps = 920\n",
      "Episode 721: Reward = -9.58, Total Steps = 921\n",
      "Episode 722: Reward = -9.35, Total Steps = 922\n",
      "Episode 723: Reward = -9.15, Total Steps = 923\n",
      "Episode 724: Reward = -8.93, Total Steps = 924\n",
      "Episode 725: Reward = -8.71, Total Steps = 925\n",
      "Episode 726: Reward = -8.56, Total Steps = 926\n",
      "Episode 727: Reward = -8.49, Total Steps = 927\n",
      "Episode 728: Reward = -8.45, Total Steps = 928\n",
      "Episode 729: Reward = -8.54, Total Steps = 929\n",
      "Episode 730: Reward = -8.64, Total Steps = 930\n",
      "Episode 731: Reward = -8.81, Total Steps = 931\n",
      "Episode 732: Reward = -8.94, Total Steps = 932\n",
      "Episode 733: Reward = -9.14, Total Steps = 933\n",
      "Episode 734: Reward = -9.41, Total Steps = 934\n",
      "Episode 735: Reward = -9.83, Total Steps = 935\n",
      "Episode 736: Reward = -9.82, Total Steps = 936\n",
      "Episode 737: Reward = -9.50, Total Steps = 937\n",
      "Episode 738: Reward = -9.26, Total Steps = 938\n",
      "Episode 739: Reward = -9.08, Total Steps = 939\n",
      "Episode 740: Reward = -8.91, Total Steps = 940\n",
      "Episode 741: Reward = -8.73, Total Steps = 941\n",
      "Episode 742: Reward = -8.55, Total Steps = 942\n",
      "Episode 743: Reward = -8.40, Total Steps = 943\n",
      "Episode 744: Reward = -8.30, Total Steps = 944\n",
      "Episode 745: Reward = -8.24, Total Steps = 945\n",
      "Episode 746: Reward = -8.22, Total Steps = 946\n",
      "Episode 747: Reward = -8.30, Total Steps = 947\n",
      "Episode 748: Reward = -8.46, Total Steps = 948\n",
      "Episode 749: Reward = -8.71, Total Steps = 949\n",
      "Episode 750: Reward = -9.02, Total Steps = 950\n",
      "Episode 751: Reward = -9.27, Total Steps = 951\n",
      "Episode 752: Reward = -9.64, Total Steps = 952\n",
      "Episode 753: Reward = -9.95, Total Steps = 953\n",
      "Episode 754: Reward = -9.67, Total Steps = 954\n",
      "Episode 755: Reward = -9.31, Total Steps = 955\n",
      "Episode 756: Reward = -8.95, Total Steps = 956\n",
      "Episode 757: Reward = -8.65, Total Steps = 957\n",
      "Episode 758: Reward = -8.40, Total Steps = 958\n",
      "Episode 759: Reward = -8.25, Total Steps = 959\n",
      "Episode 760: Reward = -8.17, Total Steps = 960\n",
      "Episode 761: Reward = -8.18, Total Steps = 961\n",
      "Episode 762: Reward = -8.27, Total Steps = 962\n",
      "Episode 763: Reward = -8.42, Total Steps = 963\n",
      "Episode 764: Reward = -8.70, Total Steps = 964\n",
      "Episode 765: Reward = -9.08, Total Steps = 965\n",
      "Episode 766: Reward = -9.53, Total Steps = 966\n",
      "Episode 767: Reward = -10.08, Total Steps = 967\n",
      "Episode 768: Reward = -9.66, Total Steps = 968\n",
      "Episode 769: Reward = -9.18, Total Steps = 969\n",
      "Episode 770: Reward = -8.71, Total Steps = 970\n",
      "Episode 771: Reward = -8.19, Total Steps = 971\n",
      "Episode 772: Reward = -7.70, Total Steps = 972\n",
      "Episode 773: Reward = -7.24, Total Steps = 973\n",
      "Episode 774: Reward = -6.89, Total Steps = 974\n",
      "Episode 775: Reward = -6.69, Total Steps = 975\n",
      "Episode 776: Reward = -6.65, Total Steps = 976\n",
      "Episode 777: Reward = -6.72, Total Steps = 977\n",
      "Episode 778: Reward = -6.96, Total Steps = 978\n",
      "Episode 779: Reward = -7.34, Total Steps = 979\n",
      "Episode 780: Reward = -7.97, Total Steps = 980\n",
      "Episode 781: Reward = -8.65, Total Steps = 981\n",
      "Episode 782: Reward = -9.46, Total Steps = 982\n",
      "Episode 783: Reward = -10.38, Total Steps = 983\n",
      "Episode 784: Reward = -9.85, Total Steps = 984\n",
      "Episode 785: Reward = -9.12, Total Steps = 985\n",
      "Episode 786: Reward = -8.50, Total Steps = 986\n",
      "Episode 787: Reward = -8.02, Total Steps = 987\n",
      "Episode 788: Reward = -7.62, Total Steps = 988\n",
      "Episode 789: Reward = -7.41, Total Steps = 989\n",
      "Episode 790: Reward = -7.35, Total Steps = 990\n",
      "Episode 791: Reward = -7.49, Total Steps = 991\n",
      "Episode 792: Reward = -7.70, Total Steps = 992\n",
      "Episode 793: Reward = -7.97, Total Steps = 993\n",
      "Episode 794: Reward = -8.31, Total Steps = 994\n",
      "Episode 795: Reward = -8.88, Total Steps = 995\n",
      "Episode 796: Reward = -9.39, Total Steps = 996\n",
      "Episode 797: Reward = -10.08, Total Steps = 997\n",
      "Episode 798: Reward = -9.80, Total Steps = 998\n",
      "Episode 799: Reward = -9.21, Total Steps = 999\n",
      "Episode 800: Reward = -8.67, Total Steps = 1000\n",
      "Episode 801: Reward = -8.08, Total Steps = 1001\n",
      "Episode 802: Reward = -7.52, Total Steps = 1002\n",
      "Episode 803: Reward = -6.92, Total Steps = 1003\n",
      "Episode 804: Reward = -6.46, Total Steps = 1004\n",
      "Episode 805: Reward = -6.23, Total Steps = 1005\n",
      "Episode 806: Reward = -6.14, Total Steps = 1006\n",
      "Episode 807: Reward = -6.20, Total Steps = 1007\n",
      "Episode 808: Reward = -6.53, Total Steps = 1008\n",
      "Episode 809: Reward = -6.96, Total Steps = 1009\n",
      "Episode 810: Reward = -7.47, Total Steps = 1010\n",
      "Episode 811: Reward = -8.17, Total Steps = 1011\n",
      "Episode 812: Reward = -8.77, Total Steps = 1012\n",
      "Episode 813: Reward = -9.63, Total Steps = 1013\n",
      "Episode 814: Reward = -10.42, Total Steps = 1014\n",
      "Episode 815: Reward = -9.82, Total Steps = 1015\n",
      "Episode 816: Reward = -8.98, Total Steps = 1016\n",
      "Episode 817: Reward = -8.23, Total Steps = 1017\n",
      "Episode 818: Reward = -7.65, Total Steps = 1018\n",
      "Episode 819: Reward = -7.17, Total Steps = 1019\n",
      "Episode 820: Reward = -6.74, Total Steps = 1020\n",
      "Episode 821: Reward = -6.49, Total Steps = 1021\n",
      "Episode 822: Reward = -6.36, Total Steps = 1022\n",
      "Episode 823: Reward = -6.42, Total Steps = 1023\n",
      "Episode 824: Reward = -6.72, Total Steps = 1024\n",
      "Episode 825: Reward = -7.10, Total Steps = 1025\n",
      "Episode 826: Reward = -7.67, Total Steps = 1026\n",
      "Episode 827: Reward = -8.13, Total Steps = 1027\n",
      "Episode 828: Reward = -8.71, Total Steps = 1028\n",
      "Episode 829: Reward = -9.34, Total Steps = 1029\n",
      "Episode 830: Reward = -10.09, Total Steps = 1030\n",
      "Episode 831: Reward = -9.92, Total Steps = 1031\n",
      "Episode 832: Reward = -9.26, Total Steps = 1032\n",
      "Episode 833: Reward = -8.64, Total Steps = 1033\n",
      "Episode 834: Reward = -8.07, Total Steps = 1034\n",
      "Episode 835: Reward = -7.45, Total Steps = 1035\n",
      "Episode 836: Reward = -7.00, Total Steps = 1036\n",
      "Episode 837: Reward = -6.57, Total Steps = 1037\n",
      "Episode 838: Reward = -6.27, Total Steps = 1038\n",
      "Episode 839: Reward = -6.11, Total Steps = 1039\n",
      "Episode 840: Reward = -6.07, Total Steps = 1040\n",
      "Episode 841: Reward = -6.18, Total Steps = 1041\n",
      "Episode 842: Reward = -6.43, Total Steps = 1042\n",
      "Episode 843: Reward = -6.81, Total Steps = 1043\n",
      "Episode 844: Reward = -7.38, Total Steps = 1044\n",
      "Episode 845: Reward = -8.11, Total Steps = 1045\n",
      "Episode 846: Reward = -9.00, Total Steps = 1046\n",
      "Episode 847: Reward = -9.78, Total Steps = 1047\n",
      "Episode 848: Reward = -10.42, Total Steps = 1048\n",
      "Episode 849: Reward = -9.73, Total Steps = 1049\n",
      "Episode 850: Reward = -8.97, Total Steps = 1050\n",
      "Episode 851: Reward = -8.25, Total Steps = 1051\n",
      "Episode 852: Reward = -7.65, Total Steps = 1052\n",
      "Episode 853: Reward = -7.25, Total Steps = 1053\n",
      "Episode 854: Reward = -6.96, Total Steps = 1054\n",
      "Episode 855: Reward = -6.81, Total Steps = 1055\n",
      "Episode 856: Reward = -6.79, Total Steps = 1056\n",
      "Episode 857: Reward = -6.92, Total Steps = 1057\n",
      "Episode 858: Reward = -7.22, Total Steps = 1058\n",
      "Episode 859: Reward = -7.77, Total Steps = 1059\n",
      "Episode 860: Reward = -8.36, Total Steps = 1060\n",
      "Episode 861: Reward = -9.23, Total Steps = 1061\n",
      "Episode 862: Reward = -10.04, Total Steps = 1062\n",
      "Episode 863: Reward = -10.21, Total Steps = 1063\n",
      "Episode 864: Reward = -9.32, Total Steps = 1064\n",
      "Episode 865: Reward = -8.52, Total Steps = 1065\n",
      "Episode 866: Reward = -7.90, Total Steps = 1066\n",
      "Episode 867: Reward = -7.32, Total Steps = 1067\n",
      "Episode 868: Reward = -6.88, Total Steps = 1068\n",
      "Episode 869: Reward = -6.53, Total Steps = 1069\n",
      "Episode 870: Reward = -6.29, Total Steps = 1070\n",
      "Episode 871: Reward = -6.17, Total Steps = 1071\n",
      "Episode 872: Reward = -6.17, Total Steps = 1072\n",
      "Episode 873: Reward = -6.32, Total Steps = 1073\n",
      "Episode 874: Reward = -6.51, Total Steps = 1074\n",
      "Episode 875: Reward = -6.88, Total Steps = 1075\n",
      "Episode 876: Reward = -7.40, Total Steps = 1076\n",
      "Episode 877: Reward = -8.09, Total Steps = 1077\n",
      "Episode 878: Reward = -8.75, Total Steps = 1078\n",
      "Episode 879: Reward = -9.50, Total Steps = 1079\n",
      "Episode 880: Reward = -10.24, Total Steps = 1080\n",
      "Episode 881: Reward = -9.86, Total Steps = 1081\n",
      "Episode 882: Reward = -9.15, Total Steps = 1082\n",
      "Episode 883: Reward = -8.54, Total Steps = 1083\n",
      "Episode 884: Reward = -7.93, Total Steps = 1084\n",
      "Episode 885: Reward = -7.43, Total Steps = 1085\n",
      "Episode 886: Reward = -7.14, Total Steps = 1086\n",
      "Episode 887: Reward = -6.97, Total Steps = 1087\n",
      "Episode 888: Reward = -6.87, Total Steps = 1088\n",
      "Episode 889: Reward = -6.88, Total Steps = 1089\n",
      "Episode 890: Reward = -7.00, Total Steps = 1090\n",
      "Episode 891: Reward = -7.32, Total Steps = 1091\n",
      "Episode 892: Reward = -7.62, Total Steps = 1092\n",
      "Episode 893: Reward = -7.97, Total Steps = 1093\n",
      "Episode 894: Reward = -8.47, Total Steps = 1094\n",
      "Episode 895: Reward = -8.98, Total Steps = 1095\n",
      "Episode 896: Reward = -9.55, Total Steps = 1096\n",
      "Episode 897: Reward = -10.13, Total Steps = 1097\n",
      "Episode 898: Reward = -9.70, Total Steps = 1098\n",
      "Episode 899: Reward = -9.18, Total Steps = 1099\n",
      "Episode 900: Reward = -8.68, Total Steps = 1100\n",
      "Episode 901: Reward = -8.24, Total Steps = 1101\n",
      "Episode 902: Reward = -7.90, Total Steps = 1102\n",
      "Episode 903: Reward = -7.64, Total Steps = 1103\n",
      "Episode 904: Reward = -7.52, Total Steps = 1104\n",
      "Episode 905: Reward = -7.49, Total Steps = 1105\n",
      "Episode 906: Reward = -7.54, Total Steps = 1106\n",
      "Episode 907: Reward = -7.75, Total Steps = 1107\n",
      "Episode 908: Reward = -8.03, Total Steps = 1108\n",
      "Episode 909: Reward = -8.31, Total Steps = 1109\n",
      "Episode 910: Reward = -8.75, Total Steps = 1110\n",
      "Episode 911: Reward = -9.33, Total Steps = 1111\n",
      "Episode 912: Reward = -9.79, Total Steps = 1112\n",
      "Episode 913: Reward = -9.96, Total Steps = 1113\n",
      "Episode 914: Reward = -9.42, Total Steps = 1114\n",
      "Episode 915: Reward = -8.89, Total Steps = 1115\n",
      "Episode 916: Reward = -8.49, Total Steps = 1116\n",
      "Episode 917: Reward = -8.19, Total Steps = 1117\n",
      "Episode 918: Reward = -8.01, Total Steps = 1118\n",
      "Episode 919: Reward = -7.93, Total Steps = 1119\n",
      "Episode 920: Reward = -7.88, Total Steps = 1120\n",
      "Episode 921: Reward = -7.85, Total Steps = 1121\n",
      "Episode 922: Reward = -7.85, Total Steps = 1122\n",
      "Episode 923: Reward = -7.96, Total Steps = 1123\n",
      "Episode 924: Reward = -8.16, Total Steps = 1124\n",
      "Episode 925: Reward = -8.41, Total Steps = 1125\n",
      "Episode 926: Reward = -8.69, Total Steps = 1126\n",
      "Episode 927: Reward = -8.95, Total Steps = 1127\n",
      "Episode 928: Reward = -9.13, Total Steps = 1128\n",
      "Episode 929: Reward = -9.49, Total Steps = 1129\n",
      "Episode 930: Reward = -9.84, Total Steps = 1130\n",
      "Episode 931: Reward = -9.81, Total Steps = 1131\n",
      "Episode 932: Reward = -9.54, Total Steps = 1132\n",
      "Episode 933: Reward = -9.33, Total Steps = 1133\n",
      "Episode 934: Reward = -9.10, Total Steps = 1134\n",
      "Episode 935: Reward = -8.89, Total Steps = 1135\n",
      "Episode 936: Reward = -8.76, Total Steps = 1136\n",
      "Episode 937: Reward = -8.67, Total Steps = 1137\n",
      "Episode 938: Reward = -8.67, Total Steps = 1138\n",
      "Episode 939: Reward = -8.76, Total Steps = 1139\n",
      "Episode 940: Reward = -8.94, Total Steps = 1140\n",
      "Episode 941: Reward = -9.24, Total Steps = 1141\n",
      "Episode 942: Reward = -9.51, Total Steps = 1142\n",
      "Episode 943: Reward = -9.95, Total Steps = 1143\n",
      "Episode 944: Reward = -9.72, Total Steps = 1144\n",
      "Episode 945: Reward = -9.38, Total Steps = 1145\n",
      "Episode 946: Reward = -9.05, Total Steps = 1146\n",
      "Episode 947: Reward = -8.74, Total Steps = 1147\n",
      "Episode 948: Reward = -8.43, Total Steps = 1148\n",
      "Episode 949: Reward = -8.16, Total Steps = 1149\n",
      "Episode 950: Reward = -8.00, Total Steps = 1150\n",
      "Episode 951: Reward = -7.95, Total Steps = 1151\n",
      "Episode 952: Reward = -7.94, Total Steps = 1152\n",
      "Episode 953: Reward = -7.96, Total Steps = 1153\n",
      "Episode 954: Reward = -7.98, Total Steps = 1154\n",
      "Episode 955: Reward = -8.12, Total Steps = 1155\n",
      "Episode 956: Reward = -8.24, Total Steps = 1156\n",
      "Episode 957: Reward = -8.57, Total Steps = 1157\n",
      "Episode 958: Reward = -8.83, Total Steps = 1158\n",
      "Episode 959: Reward = -9.13, Total Steps = 1159\n",
      "Episode 960: Reward = -9.39, Total Steps = 1160\n",
      "Episode 961: Reward = -9.62, Total Steps = 1161\n",
      "Episode 962: Reward = -9.88, Total Steps = 1162\n",
      "Episode 963: Reward = -9.74, Total Steps = 1163\n",
      "Episode 964: Reward = -9.50, Total Steps = 1164\n",
      "Episode 965: Reward = -9.24, Total Steps = 1165\n",
      "Episode 966: Reward = -8.98, Total Steps = 1166\n",
      "Episode 967: Reward = -8.79, Total Steps = 1167\n",
      "Episode 968: Reward = -8.66, Total Steps = 1168\n",
      "Episode 969: Reward = -8.51, Total Steps = 1169\n",
      "Episode 970: Reward = -8.44, Total Steps = 1170\n",
      "Episode 971: Reward = -8.47, Total Steps = 1171\n",
      "Episode 972: Reward = -8.67, Total Steps = 1172\n",
      "Episode 973: Reward = -8.78, Total Steps = 1173\n",
      "Episode 974: Reward = -8.92, Total Steps = 1174\n",
      "Episode 975: Reward = -9.10, Total Steps = 1175\n",
      "Episode 976: Reward = -9.30, Total Steps = 1176\n",
      "Episode 977: Reward = -9.51, Total Steps = 1177\n",
      "Episode 978: Reward = -9.79, Total Steps = 1178\n",
      "Episode 979: Reward = -9.89, Total Steps = 1179\n",
      "Episode 980: Reward = -9.70, Total Steps = 1180\n",
      "Episode 981: Reward = -9.54, Total Steps = 1181\n",
      "Episode 982: Reward = -9.38, Total Steps = 1182\n",
      "Episode 983: Reward = -9.26, Total Steps = 1183\n",
      "Episode 984: Reward = -9.16, Total Steps = 1184\n",
      "Episode 985: Reward = -9.10, Total Steps = 1185\n",
      "Episode 986: Reward = -9.05, Total Steps = 1186\n",
      "Episode 987: Reward = -9.06, Total Steps = 1187\n",
      "Episode 988: Reward = -9.21, Total Steps = 1188\n",
      "Episode 989: Reward = -9.32, Total Steps = 1189\n",
      "Episode 990: Reward = -9.36, Total Steps = 1190\n",
      "Episode 991: Reward = -9.39, Total Steps = 1191\n",
      "Episode 992: Reward = -9.50, Total Steps = 1192\n",
      "Episode 993: Reward = -9.57, Total Steps = 1193\n",
      "Episode 994: Reward = -9.71, Total Steps = 1194\n",
      "Episode 995: Reward = -9.86, Total Steps = 1195\n",
      "Episode 996: Reward = -9.74, Total Steps = 1196\n",
      "Episode 997: Reward = -9.55, Total Steps = 1197\n",
      "Episode 998: Reward = -9.43, Total Steps = 1198\n",
      "Episode 999: Reward = -9.42, Total Steps = 1199\n",
      "Episode 1000: Reward = -9.45, Total Steps = 1200\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_multiple_episodes(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3b3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 28.1     |\n",
      "|    ep_rew_mean        | 28.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1633     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.593   |\n",
      "|    explained_variance | 0.0306   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.273    |\n",
      "|    value_loss         | 26.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 30.7     |\n",
      "|    ep_rew_mean        | 30.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1832     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.554   |\n",
      "|    explained_variance | 0.541    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.981    |\n",
      "|    value_loss         | 5.53     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 38.2     |\n",
      "|    ep_rew_mean        | 38.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1892     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.492   |\n",
      "|    explained_variance | 0.588    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.37     |\n",
      "|    value_loss         | 5.19     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 45.2     |\n",
      "|    ep_rew_mean        | 45.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1906     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.495   |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.412   |\n",
      "|    value_loss         | 4.48     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 46.5     |\n",
      "|    ep_rew_mean        | 46.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1914     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.481   |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -1.01    |\n",
      "|    value_loss         | 12       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 44.5     |\n",
      "|    ep_rew_mean        | 44.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1909     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.502   |\n",
      "|    explained_variance | 0.629    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 1.03     |\n",
      "|    value_loss         | 29.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 48.6     |\n",
      "|    ep_rew_mean        | 48.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1926     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.516   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.0571   |\n",
      "|    value_loss         | 0.515    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 57.2     |\n",
      "|    ep_rew_mean        | 57.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1951     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.51    |\n",
      "|    explained_variance | 0.443    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.822    |\n",
      "|    value_loss         | 4.53     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 72.1     |\n",
      "|    ep_rew_mean        | 72.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1966     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.541   |\n",
      "|    explained_variance | -0.0107  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.858    |\n",
      "|    value_loss         | 3.92     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 86.8     |\n",
      "|    ep_rew_mean        | 86.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1975     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.51    |\n",
      "|    explained_variance | -0.00569 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.941    |\n",
      "|    value_loss         | 3.37     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 96.9     |\n",
      "|    ep_rew_mean        | 96.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1975     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.497   |\n",
      "|    explained_variance | -0.0013  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 1.05     |\n",
      "|    value_loss         | 3.01     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 112       |\n",
      "|    ep_rew_mean        | 112       |\n",
      "| time/                 |           |\n",
      "|    fps                | 1951      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.508    |\n",
      "|    explained_variance | -3.41e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 0.756     |\n",
      "|    value_loss         | 2.59      |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "pygame is not installed, run `pip install \"gymnasium[classic-control]\"`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:258\u001b[39m, in \u001b[36mCartPoleEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpygame\u001b[39;00m\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpygame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gfxdraw\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pygame'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDependencyNotInstalled\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m action, _states = model.predict(obs)\n\u001b[32m     18\u001b[39m obs, rewards, dones, info = vec_env.step(action)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mvec_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:104\u001b[39m, in \u001b[36mDummyVecEnv.render\u001b[39m\u001b[34m(self, mode)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Optional[np.ndarray]:\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    Gym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    they are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[32m    101\u001b[39m \n\u001b[32m    102\u001b[39m \u001b[33;03m    :param mode: The rendering type.\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:271\u001b[39m, in \u001b[36mVecEnv.render\u001b[39m\u001b[34m(self, mode)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    270\u001b[39m     \u001b[38;5;66;03m# call the render method of the environments\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     images = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# Create a big image by tiling images from subprocesses\u001b[39;00m\n\u001b[32m    273\u001b[39m     bigimg = tile_images(images)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:95\u001b[39m, in \u001b[36mDummyVecEnv.get_images\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     91\u001b[39m     warnings.warn(\n\u001b[32m     92\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe render mode is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.render_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but this method assumes it is `rgb_array` to obtain images.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     93\u001b[39m     )\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.envs]\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.envs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\gymnasium\\core.py:332\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> RenderFrame | \u001b[38;5;28mlist\u001b[39m[RenderFrame] | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\gymnasium\\core.py:332\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> RenderFrame | \u001b[38;5;28mlist\u001b[39m[RenderFrame] | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:409\u001b[39m, in \u001b[36mOrderEnforcing.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    408\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\gymnasium\\core.py:332\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> RenderFrame | \u001b[38;5;28mlist\u001b[39m[RenderFrame] | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:301\u001b[39m, in \u001b[36mPassiveEnvChecker.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checked_render \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    300\u001b[39m     \u001b[38;5;28mself\u001b[39m.checked_render = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_render_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.render()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:361\u001b[39m, in \u001b[36menv_render_passive_checker\u001b[39m\u001b[34m(env)\u001b[39m\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    356\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m env.render_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env.render_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[32m    357\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    358\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv.render_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    359\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m result = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m env.render_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    363\u001b[39m     _check_render_return(env.render_mode, result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:261\u001b[39m, in \u001b[36mCartPoleEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpygame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gfxdraw\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[32m    262\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mpygame is not installed, run `pip install \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgymnasium[classic-control]\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    263\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.screen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     pygame.init()\n",
      "\u001b[31mDependencyNotInstalled\u001b[39m: pygame is not installed, run `pip install \"gymnasium[classic-control]\"`"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3.common.policies import MlpPolicy\n",
    "from stable_baselines3.common import make_vec_env\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# Parallel environments\n",
    "env = make_vec_env('CartPole-v1', n_envs=4)\n",
    "\n",
    "model = A2C(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"a2c_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3a6bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deols\\anaconda3\\envs\\cogsat\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.39e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 398       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | 7.89e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -47.6     |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 833       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.52e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 484       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | -0.00151  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -32.7     |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 765       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.62e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 407       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | 0.000277  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -36       |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 940       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.67e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 436       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | 6.75e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -38.4     |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 923       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.69e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 431       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | -6.16e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -37.6     |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 823       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.7e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 446       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.44     |\n",
      "|    explained_variance | -1.86e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -39.8     |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 933       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.71e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 425       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -0.00018  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -26.4     |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 567       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.71e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 439       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -4.08e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -38       |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 808       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.71e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 445       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -3.58e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -37.1     |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 871       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.72e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 456       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -7.87e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -27.3     |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 760       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.73e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 465       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | 3.87e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -36.7     |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 893       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.74e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 457       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | 4.05e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -30       |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 795       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.74e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 465       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -5.6e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -31.4     |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 702       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.74e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 463       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -8.34e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -53.5     |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 850       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.75e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 470       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | -1.91e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -32.2     |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 783       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.75e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 464       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | -4.77e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -44.1     |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 860       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.75e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 469       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -28.9     |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 848       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.76e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 461       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -32.4     |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 830       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.76e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 466       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -52.2     |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 795       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.76e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 468       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -3.58e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -29.5     |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 650       |\n",
      "-------------------------------------\n",
      "(array([-0.86772656, -0.49704185, -0.9468436 ], dtype=float32), -6.675131696980968, False, False, {})\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     32\u001b[39m     obs, reward, done = result\n\u001b[32m     33\u001b[39m     info = {}  \u001b[38;5;66;03m# Assign an empty dictionary if info is not returned\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28mprint\u001b[39m(action, obs, \u001b[43mreward\u001b[49m, done, info)\n\u001b[32m     38\u001b[39m rewards.append(reward)\n\u001b[32m     39\u001b[39m steps.append(step_count)\n",
      "\u001b[31mNameError\u001b[39m: name 'reward' is not defined"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# Create the Pendulum environment\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "# Initialize the A2C model\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the model (you can adjust the number of timesteps)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Run the model and store the rewards for each step\n",
    "obs, _ = env.reset()  # Correcting for the tuple returned by reset()\n",
    "done = False\n",
    "rewards = []\n",
    "steps = []\n",
    "\n",
    "step_count = 0\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    result = env.step(action)  # Get the step result\n",
    "\n",
    "    print(result)\n",
    "\n",
    "    # If there are 4 returned values (older Gym versions)\n",
    "    if len(result) == 4:\n",
    "        obs, reward, done, info = result\n",
    "    # If there are 3 returned values (newer Gym versions)\n",
    "    elif len(result) == 3:\n",
    "        obs, reward, done = result\n",
    "        info = {}  # Assign an empty dictionary if info is not returned\n",
    "\n",
    "\n",
    "    print(action, obs, reward, done, info)\n",
    "\n",
    "    rewards.append(reward)\n",
    "    steps.append(step_count)\n",
    "    \n",
    "    # Print the reward for each step (optional)\n",
    "    print(f\"Step {step_count}, Reward: {reward}\")\n",
    "    \n",
    "    step_count += 1\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(steps, rewards, label=\"Reward per Step\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Reward vs. Step in Pendulum Environment\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogsat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
