{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ad70fa",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1644e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utils.env import CogSatEnv\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# set the seed\n",
    "seed = 42\n",
    "\n",
    "gymnasium.register(\n",
    "    id='CogSatEnv-v1',  # Use the same ID here as you used in the script\n",
    "    entry_point='env:CogSatEnv',\n",
    ")\n",
    "\n",
    "# Initialize the environment\n",
    "env_id = \"CogSatEnv-v1\"\n",
    "env = CogSatEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a63085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.LeoChannels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7598fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++===== ENV RESET+++===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'utc_time': array([1744250400], dtype=int64),\n",
       "  'leo_pos': array([-65.25349693, 131.19641504]),\n",
       "  'geo_freq': array([1.5e+09]),\n",
       "  'leo_freq': array([0.]),\n",
       "  'leo_access': array([0., 0.])},\n",
       " {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset(seed=seed)  # Reset the environment with the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a7e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_env = DummyVecEnv([lambda: env])  # Wrap the environment with DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d01854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++===== ENV RESET+++===\n",
      "++++===== ENV RESET+++===\n",
      "Action taken:  1\n",
      "Step Scenario 2.0\n",
      "Reward:  -32.4115512468957\n",
      "++++===== ENV RESET+++===\n",
      "Action taken:  7\n",
      "Step Scenario 2.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 3.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  3\n",
      "Step Scenario 4.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  3\n",
      "Step Scenario 5.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  8\n",
      "Step Scenario 6.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  1\n",
      "Step Scenario 7.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  8\n",
      "Step Scenario 8.0\n",
      "Reward:  -31.70839275978682\n",
      "Action taken:  2\n",
      "Step Scenario 9.0\n",
      "Reward:  -31.394433600868865\n",
      "Action taken:  8\n",
      "Step Scenario 10.0\n",
      "Reward:  -31.298294162502515\n",
      "Action taken:  1\n",
      "Step Scenario 11.0\n",
      "Reward:  -30.91373204662449\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "++++===== ENV RESET+++===\n",
      "Action taken:  7\n",
      "Step Scenario 2.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  1\n",
      "Step Scenario 3.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 4.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  3\n",
      "Step Scenario 5.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  0\n",
      "Step Scenario 6.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  3\n",
      "Step Scenario 7.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  1\n",
      "Step Scenario 8.0\n",
      "Reward:  -31.566784878106688\n",
      "Action taken:  7\n",
      "Step Scenario 9.0\n",
      "Reward:  -31.495557052982463\n",
      "Action taken:  8\n",
      "Step Scenario 10.0\n",
      "Reward:  -31.298294162502515\n",
      "Action taken:  5\n",
      "Step Scenario 11.0\n",
      "Reward:  -30.9944764193458\n",
      "Action taken:  0\n",
      "Step Scenario 12.0\n",
      "Reward:  -30.543498089742656\n",
      "Action taken:  4\n",
      "Step Scenario 13.0\n",
      "Reward:  -30.40782330186957\n",
      "Action taken:  4\n",
      "Step Scenario 14.0\n",
      "Reward:  -30.08306220545765\n",
      "Action taken:  9\n",
      "Step Scenario 15.0\n",
      "Reward:  -29.82994638198005\n",
      "Action taken:  4\n",
      "Step Scenario 16.0\n",
      "Reward:  -29.34744266894313\n",
      "Action taken:  4\n",
      "Step Scenario 17.0\n",
      "Reward:  -28.54868480917301\n",
      "Action taken:  4\n",
      "Step Scenario 18.0\n",
      "Reward:  -27.604425633316993\n",
      "Action taken:  2\n",
      "Step Scenario 19.0\n",
      "Reward:  -26.68618210686398\n",
      "Action taken:  4\n",
      "Step Scenario 20.0\n",
      "Reward:  -29.17348091107806\n",
      "Action taken:  4\n",
      "Step Scenario 21.0\n",
      "Reward:  -28.693519355347917\n",
      "Action taken:  4\n",
      "Step Scenario 22.0\n",
      "Reward:  -28.214843310275683\n",
      "Action taken:  4\n",
      "Step Scenario 23.0\n",
      "Reward:  -27.73402036507649\n",
      "Action taken:  8\n",
      "Step Scenario 24.0\n",
      "Reward:  -27.327687534300637\n",
      "Action taken:  1\n",
      "Step Scenario 25.0\n",
      "Reward:  -26.700038217283208\n",
      "Action taken:  4\n",
      "Step Scenario 26.0\n",
      "Reward:  -26.263300875925694\n",
      "Action taken:  4\n",
      "Step Scenario 27.0\n",
      "Reward:  -25.763287975656695\n",
      "Action taken:  4\n",
      "Step Scenario 28.0\n",
      "Reward:  -25.260172388935302\n",
      "Action taken:  4\n",
      "Step Scenario 29.0\n",
      "Reward:  -24.756061801566517\n",
      "Action taken:  8\n",
      "Step Scenario 30.0\n",
      "Reward:  -24.33012572376191\n",
      "Action taken:  2\n",
      "Step Scenario 31.0\n",
      "Reward:  -23.718434683185208\n",
      "Action taken:  4\n",
      "Step Scenario 32.0\n",
      "Reward:  -23.22047804212363\n",
      "Action taken:  4\n",
      "Step Scenario 33.0\n",
      "Reward:  -22.54829420944182\n",
      "Action taken:  9\n",
      "Step Scenario 34.0\n",
      "Reward:  -22.019448644663996\n",
      "Action taken:  4\n",
      "Step Scenario 35.0\n",
      "Reward:  -21.35277578951181\n",
      "Action taken:  4\n",
      "Step Scenario 36.0\n",
      "Reward:  -20.829534338066807\n",
      "Action taken:  4\n",
      "Step Scenario 37.0\n",
      "Reward:  -20.35828447629673\n",
      "Action taken:  4\n",
      "Step Scenario 38.0\n",
      "Reward:  -19.941718866566006\n",
      "Action taken:  4\n",
      "Step Scenario 39.0\n",
      "Reward:  -19.582832048169593\n",
      "Action taken:  4\n",
      "Step Scenario 40.0\n",
      "Reward:  -19.284594797085646\n",
      "Action taken:  4\n",
      "Step Scenario 41.0\n",
      "Reward:  -19.049672796621863\n",
      "Action taken:  4\n",
      "Step Scenario 42.0\n",
      "Reward:  -18.880193347795952\n",
      "Action taken:  4\n",
      "Step Scenario 43.0\n",
      "Reward:  -18.77755957022471\n",
      "Action taken:  5\n",
      "Step Scenario 44.0\n",
      "Reward:  -18.7604462384635\n",
      "Action taken:  4\n",
      "Step Scenario 45.0\n",
      "Reward:  -18.77410313980907\n",
      "Action taken:  4\n",
      "Step Scenario 46.0\n",
      "Reward:  -18.871604564311767\n",
      "Action taken:  4\n",
      "Step Scenario 47.0\n",
      "Reward:  -19.032638412784777\n",
      "Action taken:  4\n",
      "Step Scenario 48.0\n",
      "Reward:  -19.254236676487423\n",
      "Action taken:  4\n",
      "Step Scenario 49.0\n",
      "Reward:  -19.53278738890978\n",
      "Action taken:  1\n",
      "Step Scenario 50.0\n",
      "Reward:  -19.808981000502754\n",
      "Action taken:  4\n",
      "Step Scenario 51.0\n",
      "Reward:  -20.24409042378224\n",
      "Action taken:  4\n",
      "Step Scenario 52.0\n",
      "Reward:  -20.667965023754178\n",
      "Action taken:  4\n",
      "Step Scenario 53.0\n",
      "Reward:  -21.131405348665993\n",
      "Action taken:  4\n",
      "Step Scenario 54.0\n",
      "Reward:  -21.630244424734357\n",
      "Action taken:  4\n",
      "Step Scenario 55.0\n",
      "Reward:  -22.16073205181813\n",
      "Action taken:  4\n",
      "Step Scenario 56.0\n",
      "Reward:  -22.719693194452645\n",
      "Action taken:  4\n",
      "Step Scenario 57.0\n",
      "Reward:  -23.304688843591222\n",
      "Action taken:  4\n",
      "Step Scenario 58.0\n",
      "Reward:  -23.91418518521364\n",
      "Action taken:  4\n",
      "Step Scenario 59.0\n",
      "Reward:  -24.547750526581865\n",
      "Action taken:  4\n",
      "Step Scenario 60.0\n",
      "Reward:  -25.206307170174853\n",
      "Action taken:  4\n",
      "Step Scenario 61.0\n",
      "Reward:  -25.89246845929648\n",
      "Action taken:  4\n",
      "Step Scenario 62.0\n",
      "Reward:  -26.611026653863476\n",
      "Action taken:  4\n",
      "Step Scenario 63.0\n",
      "Reward:  -27.200651042135433\n",
      "Action taken:  4\n",
      "Step Scenario 64.0\n",
      "Reward:  -27.726441044837173\n",
      "Action taken:  4\n",
      "Step Scenario 65.0\n",
      "Reward:  -28.255669361642788\n",
      "Action taken:  4\n",
      "Step Scenario 66.0\n",
      "Reward:  -28.791845970215633\n",
      "Action taken:  3\n",
      "Step Scenario 67.0\n",
      "Reward:  -29.320594473202434\n",
      "Action taken:  4\n",
      "Step Scenario 68.0\n",
      "Reward:  -29.910862539221625\n",
      "Action taken:  4\n",
      "Step Scenario 69.0\n",
      "Reward:  -30.516111610770608\n",
      "Action taken:  4\n",
      "Step Scenario 70.0\n",
      "Reward:  -30.905747604053843\n",
      "Action taken:  4\n",
      "Step Scenario 71.0\n",
      "Reward:  -31.194252539542575\n",
      "Action taken:  4\n",
      "Step Scenario 72.0\n",
      "Reward:  -31.462256251171667\n",
      "Action taken:  4\n",
      "Step Scenario 73.0\n",
      "Reward:  -31.70969132234805\n",
      "Action taken:  4\n",
      "Step Scenario 74.0\n",
      "Reward:  -31.9365992188567\n",
      "Action taken:  4\n",
      "Step Scenario 75.0\n",
      "Reward:  -31.39385817416624\n",
      "Action taken:  4\n",
      "Step Scenario 76.0\n",
      "Reward:  -31.63508616039178\n",
      "Action taken:  4\n",
      "Step Scenario 77.0\n",
      "Reward:  -31.85003753020186\n",
      "Action taken:  4\n",
      "Step Scenario 78.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 79.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 80.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 81.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 82.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 83.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 84.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 85.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 86.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 87.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 88.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 89.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 90.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 91.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  7\n",
      "Step Scenario 92.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  0\n",
      "Step Scenario 93.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 94.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 95.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 96.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 97.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 98.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 99.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 100.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 101.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 102.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 103.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 104.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  1\n",
      "Step Scenario 105.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 106.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 107.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 108.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 109.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 110.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 111.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 112.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 113.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 114.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 115.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 116.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 117.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 118.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 119.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 120.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 121.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 122.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 123.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 124.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 125.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 126.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  8\n",
      "Step Scenario 127.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 128.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 129.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 130.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 131.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 132.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 133.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 134.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 135.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 136.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 137.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 138.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 139.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 140.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 141.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 142.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 143.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 144.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 145.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 146.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  7\n",
      "Step Scenario 147.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 148.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 149.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 150.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 151.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 152.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 153.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 154.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 155.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 156.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 157.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 158.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 159.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 160.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 161.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 162.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 163.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 164.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 165.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 166.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 167.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 168.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 169.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 170.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 171.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 172.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 173.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 174.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 175.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 176.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 177.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 178.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 179.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 180.0\n",
      "Reward:  -32.4115512468957\n",
      "Episode terminated.\n",
      "Action taken:  5\n",
      "Step Scenario 181.0\n",
      "Reward:  -32.4115512468957\n",
      "Episode terminated.\n",
      "++++===== ENV RESET+++===\n",
      "Action taken:  5\n",
      "Step Scenario 2.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 3.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 4.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 5.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 6.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 7.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 8.0\n",
      "Reward:  -31.627450862514706\n",
      "Action taken:  4\n",
      "Step Scenario 9.0\n",
      "Reward:  -31.434871430345908\n",
      "Action taken:  4\n",
      "Step Scenario 10.0\n",
      "Reward:  -31.217422377321157\n",
      "Action taken:  4\n",
      "Step Scenario 11.0\n",
      "Reward:  -30.974284558562857\n",
      "Action taken:  4\n",
      "Step Scenario 12.0\n",
      "Reward:  -30.704667583787426\n",
      "Action taken:  4\n",
      "Step Scenario 13.0\n",
      "Reward:  -30.40782330186957\n",
      "Action taken:  4\n",
      "Step Scenario 14.0\n",
      "Reward:  -30.08306220545765\n",
      "Action taken:  4\n",
      "Step Scenario 15.0\n",
      "Reward:  -29.729772443691417\n",
      "Action taken:  4\n",
      "Step Scenario 16.0\n",
      "Reward:  -29.34744266894313\n",
      "Action taken:  4\n",
      "Step Scenario 17.0\n",
      "Reward:  -28.54868480917301\n",
      "Action taken:  4\n",
      "Step Scenario 18.0\n",
      "Reward:  -27.604425633316993\n",
      "Action taken:  4\n",
      "Step Scenario 19.0\n",
      "Reward:  -26.72560668463126\n",
      "Action taken:  4\n",
      "Step Scenario 20.0\n",
      "Reward:  -29.17348091107806\n",
      "Action taken:  4\n",
      "Step Scenario 21.0\n",
      "Reward:  -28.693519355347917\n",
      "Action taken:  4\n",
      "Step Scenario 22.0\n",
      "Reward:  -28.214843310275683\n",
      "Action taken:  1\n",
      "Step Scenario 23.0\n",
      "Reward:  -27.67480617954898\n",
      "Action taken:  4\n",
      "Step Scenario 24.0\n",
      "Reward:  -27.248989568461198\n",
      "Action taken:  4\n",
      "Step Scenario 25.0\n",
      "Reward:  -26.75877402644761\n",
      "Action taken:  5\n",
      "Step Scenario 26.0\n",
      "Reward:  -26.282798634402056\n",
      "Action taken:  5\n",
      "Step Scenario 27.0\n",
      "Reward:  -25.782692387978813\n",
      "Action taken:  5\n",
      "Step Scenario 28.0\n",
      "Reward:  -25.279479118882307\n",
      "Action taken:  5\n",
      "Step Scenario 29.0\n",
      "Reward:  -24.775267318672462\n",
      "Action taken:  5\n",
      "Step Scenario 30.0\n",
      "Reward:  -24.27279860266367\n",
      "Action taken:  5\n",
      "Step Scenario 31.0\n",
      "Reward:  -23.77541293378809\n",
      "Action taken:  5\n",
      "Step Scenario 32.0\n",
      "Reward:  -23.239368816166532\n",
      "Action taken:  5\n",
      "Step Scenario 33.0\n",
      "Reward:  -22.56708039141867\n",
      "Action taken:  5\n",
      "Step Scenario 34.0\n",
      "Reward:  -21.944676452714262\n",
      "Action taken:  5\n",
      "Step Scenario 35.0\n",
      "Reward:  -21.37136173805142\n",
      "Action taken:  5\n",
      "Step Scenario 36.0\n",
      "Reward:  -20.848027757686864\n",
      "Action taken:  5\n",
      "Step Scenario 37.0\n",
      "Reward:  -20.376692550460078\n",
      "Action taken:  5\n",
      "Step Scenario 38.0\n",
      "Reward:  -19.960050342881317\n",
      "Action taken:  5\n",
      "Step Scenario 39.0\n",
      "Reward:  -19.601097153666302\n",
      "Action taken:  5\n",
      "Step Scenario 40.0\n",
      "Reward:  -19.3028050990101\n",
      "Action taken:  6\n",
      "Step Scenario 41.0\n",
      "Reward:  -19.086012731049635\n",
      "Action taken:  5\n",
      "Step Scenario 42.0\n",
      "Reward:  -18.898333094766784\n",
      "Action taken:  5\n",
      "Step Scenario 43.0\n",
      "Reward:  -18.795685101165063\n",
      "Action taken:  5\n",
      "Step Scenario 44.0\n",
      "Reward:  -18.7604462384635\n",
      "Action taken:  5\n",
      "Step Scenario 45.0\n",
      "Reward:  -18.792243956657614\n",
      "Action taken:  5\n",
      "Step Scenario 46.0\n",
      "Reward:  -18.889774573929827\n",
      "Action taken:  5\n",
      "Step Scenario 47.0\n",
      "Reward:  -19.05085126839674\n",
      "Action taken:  5\n",
      "Step Scenario 48.0\n",
      "Reward:  -19.272505156878296\n",
      "Action taken:  5\n",
      "Step Scenario 49.0\n",
      "Reward:  -19.551123174607184\n",
      "Action taken:  4\n",
      "Step Scenario 50.0\n",
      "Reward:  -19.864200199237942\n",
      "Action taken:  4\n",
      "Step Scenario 51.0\n",
      "Reward:  -20.24409042378224\n",
      "Action taken:  4\n",
      "Step Scenario 52.0\n",
      "Reward:  -20.667965023754178\n",
      "Action taken:  4\n",
      "Step Scenario 53.0\n",
      "Reward:  -21.131405348665993\n",
      "Action taken:  4\n",
      "Step Scenario 54.0\n",
      "Reward:  -21.630244424734357\n",
      "Action taken:  4\n",
      "Step Scenario 55.0\n",
      "Reward:  -22.16073205181813\n",
      "Action taken:  4\n",
      "Step Scenario 56.0\n",
      "Reward:  -22.719693194452645\n",
      "Action taken:  4\n",
      "Step Scenario 57.0\n",
      "Reward:  -23.304688843591222\n",
      "Action taken:  5\n",
      "Step Scenario 58.0\n",
      "Reward:  -23.93342181980357\n",
      "Action taken:  5\n",
      "Step Scenario 59.0\n",
      "Reward:  -24.567094768915993\n",
      "Action taken:  5\n",
      "Step Scenario 60.0\n",
      "Reward:  -25.22575630570053\n",
      "Action taken:  5\n",
      "Step Scenario 61.0\n",
      "Reward:  -25.912018950023395\n",
      "Action taken:  5\n",
      "Step Scenario 62.0\n",
      "Reward:  -26.630674250853602\n",
      "Action taken:  5\n",
      "Step Scenario 63.0\n",
      "Reward:  -27.220390892195695\n",
      "Action taken:  5\n",
      "Step Scenario 64.0\n",
      "Reward:  -27.74626779012401\n",
      "Action taken:  5\n",
      "Step Scenario 65.0\n",
      "Reward:  -28.27557723161955\n",
      "Action taken:  5\n",
      "Step Scenario 66.0\n",
      "Reward:  -28.81182886532413\n",
      "Action taken:  5\n",
      "Step Scenario 67.0\n",
      "Reward:  -29.36069378563637\n",
      "Action taken:  5\n",
      "Step Scenario 68.0\n",
      "Reward:  -29.930976238797754\n",
      "Action taken:  5\n",
      "Step Scenario 69.0\n",
      "Reward:  -30.53628077694293\n",
      "Action taken:  5\n",
      "Step Scenario 70.0\n",
      "Reward:  -30.925965496706084\n",
      "Action taken:  5\n",
      "Step Scenario 71.0\n",
      "Reward:  -31.214512390079847\n",
      "Action taken:  5\n",
      "Step Scenario 72.0\n",
      "Reward:  -31.48255130196773\n",
      "Action taken:  5\n",
      "Step Scenario 73.0\n",
      "Reward:  -31.73001486053039\n",
      "Action taken:  5\n",
      "Step Scenario 74.0\n",
      "Reward:  -31.95694460506067\n",
      "Action taken:  5\n",
      "Step Scenario 75.0\n",
      "Reward:  -31.414196893817177\n",
      "Action taken:  5\n",
      "Step Scenario 76.0\n",
      "Reward:  -31.655444692638326\n",
      "Action taken:  5\n",
      "Step Scenario 77.0\n",
      "Reward:  -31.87040788219798\n",
      "Action taken:  5\n",
      "Step Scenario 78.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 79.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 80.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 81.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 82.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 83.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 84.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 85.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 86.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 87.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 88.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 89.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 90.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 91.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 92.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 93.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 94.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 95.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 96.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 97.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  2\n",
      "Step Scenario 98.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 99.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  8\n",
      "Step Scenario 100.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 101.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 102.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 103.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 104.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 105.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 106.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 107.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 108.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 109.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 110.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 111.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 112.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 113.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 114.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 115.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 116.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 117.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 118.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 119.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 120.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 121.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 122.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 123.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 124.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 125.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 126.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 127.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 128.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 129.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  0\n",
      "Step Scenario 130.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 131.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 132.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 133.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 134.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 135.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 136.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 137.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 138.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 139.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 140.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 141.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 142.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 143.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 144.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 145.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 146.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 147.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 148.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 149.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 150.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  1\n",
      "Step Scenario 151.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 152.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 153.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 154.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 155.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 156.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 157.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 158.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 159.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 160.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 161.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 162.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 163.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 164.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 165.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 166.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 167.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 168.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 169.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 170.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 171.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 172.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  0\n",
      "Step Scenario 173.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 174.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 175.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  4\n",
      "Step Scenario 176.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  0\n",
      "Step Scenario 177.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 178.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 179.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 180.0\n",
      "Reward:  -32.4115512468957\n",
      "Episode terminated.\n",
      "Action taken:  5\n",
      "Step Scenario 181.0\n",
      "Reward:  -32.4115512468957\n",
      "Episode terminated.\n",
      "++++===== ENV RESET+++===\n",
      "Action taken:  5\n",
      "Step Scenario 2.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 3.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 4.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 5.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 6.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 7.0\n",
      "Reward:  -32.4115512468957\n",
      "Action taken:  5\n",
      "Step Scenario 8.0\n",
      "Reward:  -31.647680560058063\n",
      "Action taken:  5\n",
      "Step Scenario 9.0\n",
      "Reward:  -31.455096120848935\n",
      "Action taken:  5\n",
      "Step Scenario 10.0\n",
      "Reward:  -31.237634551219855\n",
      "Action taken:  5\n",
      "Step Scenario 11.0\n",
      "Reward:  -30.9944764193458\n",
      "Action taken:  5\n",
      "Step Scenario 12.0\n",
      "Reward:  -30.724831052674205\n",
      "Action taken:  5\n",
      "Step Scenario 13.0\n",
      "Reward:  -30.42795002625141\n",
      "Saving MATLAB Data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch_length = 180 ## got through experiment\n",
    "epoch_numbers = 100\n",
    "\n",
    "total_steps = epoch_length * epoch_numbers\n",
    "\n",
    "# Optional: Check the environment\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# Instantiate the model\n",
    "model = DQN(\n",
    "    policy=\"MultiInputPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=50000,\n",
    "    learning_starts=10,\n",
    "    batch_size=16,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=total_steps)\n",
    "# measure perofmance of training\n",
    "# Save the model\n",
    "model.save(\"dqn_cogsat\")\n",
    "env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e1e2570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deols\\anaconda3\\envs\\drltest\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RejectedExecutionError",
     "evalue": "MATLAB has already terminated",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRejectedExecutionError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run it in sepratae file\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Evaluate the agent\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m mean_reward, std_reward = \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMean reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m +/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\drltest\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:84\u001b[39m, in \u001b[36mevaluate_policy\u001b[39m\u001b[34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[39m\n\u001b[32m     82\u001b[39m current_rewards = np.zeros(n_envs)\n\u001b[32m     83\u001b[39m current_lengths = np.zeros(n_envs, dtype=\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m observations = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m states = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     86\u001b[39m episode_starts = np.ones((env.num_envs,), dtype=\u001b[38;5;28mbool\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\drltest\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:78\u001b[39m, in \u001b[36mDummyVecEnv.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m     77\u001b[39m     maybe_options = {\u001b[33m\"\u001b[39m\u001b[33moptions\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     obs, \u001b[38;5;28mself\u001b[39m.reset_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_seeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmaybe_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_obs(env_idx, obs)\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\Work\\Github\\CogSat-DRL\\DQN\\utils\\env.py:184\u001b[39m, in \u001b[36mreset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.eng.eval(\u001b[33m\"\u001b[39m\u001b[33mSaveData\u001b[39m\u001b[33m\"\u001b[39m, nargout=\u001b[32m0\u001b[39m)\n\u001b[32m    182\u001b[39m     truncated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m next_observation, reward, terminated, truncated, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\drltest\\Lib\\site-packages\\matlab\\engine\\matlabengine.py:40\u001b[39m, in \u001b[36mMatlabFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__validate_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     nargs = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mnargout\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(nargs, \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deols\\anaconda3\\envs\\drltest\\Lib\\site-packages\\matlab\\engine\\matlabengine.py:75\u001b[39m, in \u001b[36mMatlabFunc.__validate_engine\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__validate_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._engine() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._engine()._check_matlab():\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m RejectedExecutionError(pythonengine.getMessage(\u001b[33m'\u001b[39m\u001b[33mMatlabTerminated\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[31mRejectedExecutionError\u001b[39m: MATLAB has already terminated"
     ]
    }
   ],
   "source": [
    "# Run it in sepratae file\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy_env = DummyVecEnv([lambda: env])  # Wrap the environment with DummyVecEnv\n",
    "\n",
    "epoch_length = 884 ## got through experiment\n",
    "epoch_numbers = 100\n",
    "\n",
    "# Set up the checkpoint callback\n",
    "checkpoint_callback = CheckpointCallback(save_freq=epoch_length, save_path='./logs/', name_prefix='rl_model_A2C')\n",
    "\n",
    "# Specify the policy network architecture, here we are using the default MIP\n",
    "model = A2C(\"MultiInputPolicy\", env, ent_coef=0.01, verbose=1, tensorboard_log=\"./a2c_leogeo_tensorboard/\",\n",
    "            seed=seed, learning_rate=0.0001)\n",
    "\n",
    "# Define the total number of timesteps to train the model\n",
    "total_timesteps = epoch_length*epoch_numbers\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=total_timesteps, callback=checkpoint_callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"a2c_leogeoenv_1\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogsat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
